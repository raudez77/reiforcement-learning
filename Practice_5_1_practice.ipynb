{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import gymnasium as gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Discretization : $ \n",
    "- Discretization is the process of transforming continuous data or variables into discrete or categorical data or variables. In other words, it involves breaking down a continuous variable into distinct groups or categories. This is often done in order to simplify data analysis, as working with discrete values can be more efficient and easier to interpret than working with continuous values. Discretization is commonly used in fields such as statistics, data analysis, and machine learning, where continuous data must be converted into a form that can be processed by algorithms or models. There are various methods of discretization, including binning, clustering, and decision trees.\n",
    "\n",
    "#### $ Classes \\ and \\ Functions $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedEnvironment(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    class that discretize continuos data into categorical \n",
    "\n",
    "    arguments:\n",
    "    - env : gym.make environment object\n",
    "    - n_bins: int, number of bins to discretize\n",
    "\n",
    "    return :\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.make, n_bins : int = 10):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins \n",
    "\n",
    "        # discretize observation space\n",
    "        self.high = env.observation_space.high\n",
    "        self.low = env.observation_space.low\n",
    "        self.observation_space = gym.spaces.Discrete(n_bins ** len(self.high))   \n",
    "        # ============ Reason ===========\n",
    "        # We calculate the total number of possible states by taking the product of the number of bins for each dimension.\n",
    "        # For example, if n_bins is 10 and there are two dimensions in the observation space, then the total number of possible states is 10 ** 2 = 100.\n",
    "        # ===============================\n",
    "\n",
    "        # Define Bins for each dimension         \n",
    "        self.observation_bins = [np.linspace(self.low[i], self.high[i], n_bins + 1)[1:-1] for i in range(len(self.low))]    \n",
    "        # ============ Reason ===========\n",
    "        # low and high are arrays that represent the lower and upper bounds of each dimension\n",
    "        # np.linspace takes three arguments: the start value (low[i]), the end value (high[i]), and the number of intervals (n_bins + 1) between the start and end values. \n",
    "        # We add 1 to n_bins because we want to include both the lower and upper bounds in the bins.\n",
    "        # We then select only the inner bins (excluding the lower and upper bounds) using the slicing notation [1:-1].\n",
    "        # We do this for each dimension in the observation space by iterating over range(len(low))   \n",
    "        #  ===========================================================================================      \n",
    "                                                                                       \n",
    "        # Define action space  \n",
    "        self.action_space = gym.spaces.Discrete(3) # 3 discrete actions: push left, do nothing, push right\n",
    "         \n",
    "    def _discretize_observation(self, obs):\n",
    "        \"\"\" discretize the space\n",
    "        arguments:\n",
    "        - obs : observation space\n",
    "        \n",
    "        return \n",
    "        - state\n",
    "        \"\"\"\n",
    "\n",
    "        # convert continuous spaces to discrete\n",
    "        state = 0\n",
    "        for i, b in enumerate(self.observation_bins):\n",
    "            state += np.digitize(obs[i], b) * ((self.n_bins) ** i)\n",
    "        return state\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()[0]\n",
    "        return self._discretize_observation(obs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self._discretize_observation(observation), reward, terminated, truncated, info\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\" Q-Learning agent action on a continuous space\n",
    "\n",
    "    Arguments:\n",
    "        - environment : gym.make environment Object \n",
    "        - alpha : float, Learning Rate \n",
    "        - gamma : float, Discount Factor \n",
    "        - exploration_rate: float, probability of taking a random action\n",
    "        - epsilon_decay_rate: float, how quickly epsilon should decay \n",
    "        - discretization_bins:  int, number of bins to discretize the observation space\n",
    "    \n",
    "    Important Formulas\n",
    "        `Q*(s, a) = R(s, a) + gamma * max(a')`\n",
    "\n",
    "        ` The update rule is: Q(s_t, a_t) <- Q(s_t, a_t) + alpha * (r_t + gamma * max[a'](Q(s_{t+1}, a')) - Q(s_t, a_t))`\n",
    "    \"\"\"\n",
    "    def __init__ (self, environment:gym.make, alpha:float = 0.01, gamma:float=0.99, \n",
    "                  exploration_rate:float =1.0, epsilon_decay_rate:float =0.9995, min_epsilon:float=.01, discretization_bins:int=10):\n",
    "        \n",
    "        # Set Environment & Q-Table Parameters\n",
    "        self.env = DiscretizedEnvironment(env=environment, n_bins = discretization_bins)\n",
    "        self.Qtable = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "\n",
    "        # Set Learning Parameters \n",
    "        self.alpha =  alpha # Learning Rate\n",
    "        self.gamma = gamma # Discount Factor\n",
    "\n",
    "        # Set Exploration Parameters\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discretization_bins = discretization_bins\n",
    "\n",
    "    def act (self, state:int):\n",
    "        \"\"\" Select an action using epsilon-greedy policy selection\n",
    "        \n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "        \n",
    "        Return:\n",
    "        - action: int, selected action \n",
    "        \"\"\"\n",
    "\n",
    "        # Choosing a action : it wil becoming less exploratory once it gets more experience \n",
    "        if np.random.uniform () < self.exploration_rate:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # np.argmax, return the index of the action with the highest value \n",
    "            action = np.argmax(self.Qtable[state])\n",
    "        \n",
    "        return action \n",
    "\n",
    "    def learn_and_update (self, state:int, action:int, reward:float, next_state:int, is_done:bool):\n",
    "        \"\"\" Update Q-table using Q-learning algorithm\n",
    "        \n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "        - action: int, current action\n",
    "        - reward: float, reward for current state-action pair\n",
    "        - next_state: int, next state\n",
    "        - is_done: bool, whether the episode is terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # Get Current Q-Value\n",
    "        current_q_value = self.Qtable[state, action]\n",
    "\n",
    "        # compute Maximum Q-value for the Next State in the Qtable\n",
    "        max_next_q_value = np.max(self.Qtable[next_state])\n",
    "\n",
    "        # Compute the TD : temporal difference target \n",
    "        TD_target= reward + self.gamma * max_next_q_value * (not is_done)\n",
    "\n",
    "        # Compute the TD_error :  temporal-difference error\n",
    "        TD_error = TD_target - current_q_value\n",
    "\n",
    "        # Updating Q-table\n",
    "        self.Qtable[state, action] += self.alpha * TD_error\n",
    "\n",
    "    def train(self, num_episode:int):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes\n",
    "        \n",
    "        Arguments:\n",
    "        - num_episode: int, number of episodes\n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        max_avg_score = -np.inf\n",
    "        for episode in range(1, num_episode+1):\n",
    "    \n",
    "            # Reset & start Environment \n",
    "            state = self.env.reset()\n",
    "\n",
    "            # decay exploration \n",
    "            self.exploration_rate *= self.epsilon_decay_rate\n",
    "            self.exploration_rate = max(self.exploration_rate, self.min_epsilon)\n",
    "           \n",
    "            episode_reward = 0.0\n",
    "            done = False\n",
    "\n",
    "            # Starting Training \n",
    "            while not done:\n",
    "                # Select action & step \n",
    "                action = self.act(state=state)\n",
    "                next_state, reward, done, _ , _ = self.env.step(action)\n",
    "\n",
    "                # Update Q-Values \n",
    "                episode_reward += reward\n",
    "                self.learn_and_update(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Set Next State as Current State\n",
    "                state = next_state\n",
    "\n",
    "            scores.append(episode_reward)\n",
    "            if len (scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode:{episode}/{num_episode} | Max Average Score:{max_avg_score}\", end=\"\\r\")\n",
    "\n",
    "        return self.Qtable\n",
    "    \n",
    "                \n",
    "    def test(self, test_env: gym.make, num_episodes:int):\n",
    "        \"\"\"\n",
    "        Test the agent for a specified number of episodes. It will automatically extract the QTable from the training step\n",
    "\n",
    "        Arguments:\n",
    "        - test_env : gym.make object environment\n",
    "        - num_episodes: int, number of episodes to test for\n",
    "\n",
    "        Returns:\n",
    "        - total_reward: float, total reward earned over all episodes\n",
    "        \"\"\"\n",
    "        env_test = DiscretizedEnvironment(env=test_env, n_bins = self.discretization_bins)\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            num_actions = 0\n",
    "            # reset the environment and get initial state\n",
    "            state = env_test.reset()\n",
    "            \n",
    "            # loop until episode is finished\n",
    "            done = False\n",
    "            while not done:\n",
    "                # select an action\n",
    "                action = np.argmax(self.Qtable[state])\n",
    "                \n",
    "                # step the environment\n",
    "                next_state, reward, done, _, _ = env_test.step(action)\n",
    "                \n",
    "                # update total reward\n",
    "                total_reward += reward\n",
    "                \n",
    "                # set next state as current state\n",
    "                state = next_state\n",
    "                num_actions += 1\n",
    "            print(f\"Number of action needed to solve the environment : {num_actions} in episode {episode + 1}\")\n",
    "                \n",
    "        return total_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ MountainCar \\ Problem $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Environment Parameters\n",
    "ENV = {'id' :\"MountainCar-v0\", 'render_mode':None}\n",
    "env = gym.make(**ENV)\n",
    "\n",
    "# Training Environment\n",
    "QAgent = QLearningAgent(env)   \n",
    "Qtable = QAgent.train(num_episode=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of action needed to solve the environment : 105 in episode 1\n",
      "Number of action needed to solve the environment : 166 in episode 2\n",
      "Number of action needed to solve the environment : 167 in episode 3\n",
      "Number of action needed to solve the environment : 141 in episode 4\n",
      "Number of action needed to solve the environment : 144 in episode 5\n",
      "Number of action needed to solve the environment : 146 in episode 6\n",
      "Number of action needed to solve the environment : 167 in episode 7\n",
      "Number of action needed to solve the environment : 122 in episode 8\n",
      "Number of action needed to solve the environment : 101 in episode 9\n",
      "Number of action needed to solve the environment : 154 in episode 10\n",
      "Number of action needed to solve the environment : 106 in episode 11\n",
      "Number of action needed to solve the environment : 97 in episode 12\n",
      "Number of action needed to solve the environment : 122 in episode 13\n",
      "Number of action needed to solve the environment : 98 in episode 14\n",
      "Number of action needed to solve the environment : 148 in episode 15\n",
      "Number of action needed to solve the environment : 106 in episode 16\n",
      "Number of action needed to solve the environment : 106 in episode 17\n",
      "Number of action needed to solve the environment : 158 in episode 18\n",
      "Number of action needed to solve the environment : 122 in episode 19\n",
      "Number of action needed to solve the environment : 148 in episode 20\n",
      "Number of action needed to solve the environment : 147 in episode 21\n",
      "Number of action needed to solve the environment : 102 in episode 22\n",
      "Number of action needed to solve the environment : 146 in episode 23\n",
      "Number of action needed to solve the environment : 155 in episode 24\n",
      "Number of action needed to solve the environment : 119 in episode 25\n",
      "Number of action needed to solve the environment : 122 in episode 26\n",
      "Number of action needed to solve the environment : 168 in episode 27\n",
      "Number of action needed to solve the environment : 150 in episode 28\n",
      "Number of action needed to solve the environment : 166 in episode 29\n",
      "Number of action needed to solve the environment : 100 in episode 30\n",
      "Number of action needed to solve the environment : 123 in episode 31\n",
      "Number of action needed to solve the environment : 105 in episode 32\n",
      "Number of action needed to solve the environment : 154 in episode 33\n",
      "Number of action needed to solve the environment : 106 in episode 34\n",
      "Number of action needed to solve the environment : 155 in episode 35\n",
      "Number of action needed to solve the environment : 141 in episode 36\n",
      "Number of action needed to solve the environment : 147 in episode 37\n",
      "Number of action needed to solve the environment : 155 in episode 38\n",
      "Number of action needed to solve the environment : 102 in episode 39\n",
      "Number of action needed to solve the environment : 122 in episode 40\n",
      "Number of action needed to solve the environment : 140 in episode 41\n",
      "Number of action needed to solve the environment : 101 in episode 42\n",
      "Number of action needed to solve the environment : 98 in episode 43\n",
      "Number of action needed to solve the environment : 157 in episode 44\n",
      "Number of action needed to solve the environment : 106 in episode 45\n",
      "Number of action needed to solve the environment : 155 in episode 46\n",
      "Number of action needed to solve the environment : 121 in episode 47\n",
      "Number of action needed to solve the environment : 96 in episode 48\n",
      "Number of action needed to solve the environment : 119 in episode 49\n",
      "Number of action needed to solve the environment : 106 in episode 50\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "ENV_TEST = {'id' :\"MountainCar-v0\", 'render_mode':'human'}\n",
    "test_env = gym.make(**ENV_TEST)\n",
    "\n",
    "QAgent = QLearningAgent(env)  \n",
    "QAgent.test(test_env=test_env, num_episodes=50)\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
