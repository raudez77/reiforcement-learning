{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import gymnasium as gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Discretization : $ \n",
    "- Discretization is the process of transforming continuous data or variables into discrete or categorical data or variables. In other words, it involves breaking down a continuous variable into distinct groups or categories. This is often done in order to simplify data analysis, as working with discrete values can be more efficient and easier to interpret than working with continuous values. Discretization is commonly used in fields such as statistics, data analysis, and machine learning, where continuous data must be converted into a form that can be processed by algorithms or models. There are various methods of discretization, including binning, clustering, and decision trees.\n",
    "\n",
    "#### $ Classes \\ and \\ Functions $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:20 | Max Average Score:-45562.8\n",
      "Episode:30 | Max Average Score:-23675.7\n",
      "Episode:40 | Max Average Score:-23675.7\n",
      "Episode:50 | Max Average Score:-20351.3\n",
      "Episode:60 | Max Average Score:-20351.3\n",
      "Episode:70 | Max Average Score:-18424.5\n",
      "Episode:80 | Max Average Score:-18424.5\n",
      "Episode:90 | Max Average Score:-16441.0\n",
      "Episode:100 | Max Average Score:-16441.0\n"
     ]
    }
   ],
   "source": [
    "class DiscretizedEnvironment(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    class that discretize continuos data into categorical \n",
    "\n",
    "    arguments:\n",
    "    - env : gym.make environment object\n",
    "    - n_bins: int, number of bins to discretize\n",
    "\n",
    "    return :\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.make, n_bins : int = 10):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins \n",
    "\n",
    "        # discretize observation space\n",
    "        self.high = env.observation_space.high\n",
    "        self.low = env.observation_space.low\n",
    "        self.observation_space = gym.spaces.Discrete(n_bins ** len(self.high))   \n",
    "        # ============ Reason ===========\n",
    "        # We calculate the total number of possible states by taking the product of the number of bins for each dimension.\n",
    "        # For example, if n_bins is 10 and there are two dimensions in the observation space, then the total number of possible states is 10 ** 2 = 100.\n",
    "        # ===============================\n",
    "\n",
    "        # Define Bins for each dimension         \n",
    "        self.observation_bins = [np.linspace(self.low[i], self.high[i], n_bins + 1)[1:-1] for i in range(len(self.low))]    \n",
    "        # ============ Reason ===========\n",
    "        # low and high are arrays that represent the lower and upper bounds of each dimension\n",
    "        # np.linspace takes three arguments: the start value (low[i]), the end value (high[i]), and the number of intervals (n_bins + 1) between the start and end values. \n",
    "        # We add 1 to n_bins because we want to include both the lower and upper bounds in the bins.\n",
    "        # We then select only the inner bins (excluding the lower and upper bounds) using the slicing notation [1:-1].\n",
    "        # We do this for each dimension in the observation space by iterating over range(len(low))   \n",
    "        #  ===========================================================================================      \n",
    "                                                                                       \n",
    "        # Define action space  \n",
    "        self.action_space = gym.spaces.Discrete(3) # 3 discrete actions: push left, do nothing, push right\n",
    "         \n",
    "    def _discretize_observation(self, obs):\n",
    "        \"\"\" discretize the space\n",
    "        \n",
    "        arguments:\n",
    "        - obs : observation space\n",
    "        \n",
    "        return \n",
    "        - state\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # convert continuous spaces to discrete\n",
    "        state = 0\n",
    "        for i, b in enumerate(self.observation_bins):\n",
    "            state += np.digitize(obs[i], b) * ((self.n_bins) ** i)\n",
    "        return state\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()[0]\n",
    "        return self._discretize_observation(obs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self._discretize_observation(observation), reward, terminated, truncated, info\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\" Q-Learning agent action on a continuous space\n",
    "\n",
    "    Arguments:\n",
    "        - environment : gym.make environment Object \n",
    "        - alpha : float, Learning Rate \n",
    "        - gamma : float, Discount Factor \n",
    "        - exploration_rate: float, probability of taking a random action\n",
    "        - epsilon_decay_rate: float, how quickly epsilon should decay \n",
    "        - discretization_bins:  int, number of bins to discretize the observation space\n",
    "\n",
    "\n",
    "    \n",
    "    Important Formulas\n",
    "        `Q*(s, a) = R(s, a) + gamma * max(a')`\n",
    "\n",
    "        ` The update rule is: Q(s_t, a_t) <- Q(s_t, a_t) + alpha * (r_t + gamma * max[a'](Q(s_{t+1}, a')) - Q(s_t, a_t))`\n",
    "    \"\"\"\n",
    "    def __init__ (self, environment:gym.make, alpha:float = 0.1, gamma:float=0.99, \n",
    "                  exploration_rate:float =1.0, epsilon_decay_rate:float =0.99, discretization_bins:int=10):\n",
    "        \n",
    "        # Set Environment & Q-Table Parameters\n",
    "        self.env = DiscretizedEnvironment(env=environment, n_bins = discretization_bins)\n",
    "        self.Qtable = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "\n",
    "        # Set Learning Parameters \n",
    "        self.alpha =  alpha # Learning Rate\n",
    "        self.gamma = gamma # Discount Factor\n",
    "\n",
    "        # Set Exploration Parameters\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "\n",
    "    def act (self, state:int):\n",
    "        \"\"\" Select an action using epsilon-greedy policy selection\n",
    "        \n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "        \n",
    "        Return:\n",
    "        - action: int, selected action \n",
    "        \"\"\"\n",
    "\n",
    "        # Choosing a action : it wil becoming less exploratory once it gets more experience \n",
    "        if np.random.uniform () < self.exploration_rate:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # np.argmax, return the index of the action with the highest value \n",
    "            action = np.argmax(self.Qtable[state])\n",
    "        \n",
    "        return action \n",
    "\n",
    "    def learn_and_update (self, state:int, action:int, reward:float, next_state:int, is_done:bool):\n",
    "        \"\"\" Update Q-table using Q-learning algorithm\n",
    "        \n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "        - action: int, current action\n",
    "        - reward: float, reward for current state-action pair\n",
    "        - next_state: int, next state\n",
    "        - is_done: bool, whether the episode is terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # Get Current Q-Value\n",
    "        current_q_value = self.Qtable[state, action]\n",
    "\n",
    "        # compute Maximum Q-value for the Next State in the Qtable\n",
    "        max_next_q_value = np.max(self.Qtable[next_state])\n",
    "\n",
    "        # Compute the TD : temporal difference target \n",
    "        TD_target= reward + self.gamma * max_next_q_value * (not is_done)\n",
    "\n",
    "        # Compute the TD_error :  temporal-difference error\n",
    "        TD_error = TD_target - current_q_value\n",
    "\n",
    "        # Updating Q-table\n",
    "        self.Qtable[state, action] += self.alpha * TD_error\n",
    "\n",
    "\n",
    "    def train(self, num_episode:int):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes\n",
    "        \n",
    "        Arguments:\n",
    "        - num_episode: int, number of episodes\n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        max_avg_score = -np.inf\n",
    "        for episode in range(1, num_episode+1):\n",
    "    \n",
    "            # Reset & start Environment \n",
    "            state = self.env.reset()\n",
    "\n",
    "            # decay exploration \n",
    "            self.exploration_rate *= self.exploration_rate\n",
    "            episode_reward = 0.0\n",
    "            done = False\n",
    "\n",
    "            # Starting Training \n",
    "            while not done:\n",
    "                # Select action & step \n",
    "                action = self.act(state=state)\n",
    "                next_state, reward, done, _ , _ = self.env.step(action)\n",
    "\n",
    "                # Update Q-Values \n",
    "                episode_reward += reward\n",
    "                self.learn_and_update(state, action, reward, next_state, done)\n",
    "\n",
    "                # Set Next State as Current State\n",
    "                state = next_state\n",
    "\n",
    "            scores.append(episode_reward)\n",
    "\n",
    "            if len (scores) > 10:\n",
    "                avg_score = np.mean(scores[-10:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "\n",
    "                if episode % 10 == 0:\n",
    "                    print(f\"Episode:{episode} | Max Average Score:{max_avg_score}\")\n",
    "                    sys.stdout.flush()\n",
    "    \n",
    "ENV = {'id' :\"MountainCar-v0\", 'render_mode':None}\n",
    "env = gym.make(**ENV)\n",
    "QAgent = QLearningAgent(env)   \n",
    "QAgent.train(num_episode=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-learning agent that learns to solve the MountainCar problem.\n",
    "\n",
    "    Arguments:\n",
    "    - env: gym.make environment object\n",
    "    - learning_rate: float, learning rate for updating Q-values\n",
    "    - discount_factor: float, discount factor for future rewards\n",
    "    - exploration_rate: float, probability of taking a random action\n",
    "    - exploration_decay_rate: float, rate at which the exploration rate decays over time\n",
    "    - discretization_bins: int, number of bins to discretize the observation space\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, \n",
    "                 exploration_decay_rate=0.99, discretization_bins=10):\n",
    "        \n",
    "        # set the environment and discretization\n",
    "        self.env = DiscretizedEnvironment(env, n_bins=discretization_bins)\n",
    "        \n",
    "        # initialize Q-values table\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "        # set learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # set exploration parameters\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using an epsilon-greedy policy.\n",
    "\n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "\n",
    "        Returns:\n",
    "        - action: int, selected action\n",
    "        \"\"\"\n",
    "        # choose a random action with probability of exploration_rate\n",
    "        if np.random.uniform() < self.exploration_rate:\n",
    "            action = self.env.action_space.sample()\n",
    "        # otherwise, choose the action with highest Q-value for the current state\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-values using Q-learning update rule.\n",
    "\n",
    "        Arguments:\n",
    "        - state: int, current state\n",
    "        - action: int, selected action\n",
    "        - reward: float, received reward\n",
    "        - next_state: int, next state\n",
    "        - done: bool, whether the episode is finished\n",
    "        \"\"\"\n",
    "        # update Q-value for the current state and action\n",
    "        current_q = self.q_table[state, action]\n",
    "        max_q = np.max(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * max_q * (not done)\n",
    "        td_error = td_target - current_q\n",
    "        self.q_table[state, action] += self.learning_rate * td_error\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes.\n",
    "\n",
    "        Arguments:\n",
    "        - num_episodes: int, number of episodes to train for\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            # reset the environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # decay exploration rate\n",
    "            self.exploration_rate *= self.exploration_decay_rate\n",
    "            episode_reward = 0.0\n",
    "            \n",
    "            # loop until episode is finished\n",
    "            done = False\n",
    "            while not done:\n",
    "                # select an action and step the environment\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # update Q-values\n",
    "                self.learn(state, action, reward, next_state, done)\n",
    "                \n",
    "                # set next state as current state\n",
    "                state = next_state\n",
    "                \n",
    "    def test(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Test the agent for a specified number of episodes.\n",
    "\n",
    "        Arguments:\n",
    "        - num_episodes: int, number of episodes to test for\n",
    "\n",
    "        Returns:\n",
    "        - total_reward: float, total reward earned over all episodes\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # reset the environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # loop until episode is finished\n",
    "            done = False\n",
    "            while not done:\n",
    "                # select an action\n",
    "                action = np.argmax(self.q_table[state])\n",
    "                \n",
    "                # step the environment\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # update total reward\n",
    "                total_reward += reward\n",
    "                \n",
    "                # set next state as current state\n",
    "                state = next_state\n",
    "                \n",
    "        return total_reward\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ MountainCar \\ Problem $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
