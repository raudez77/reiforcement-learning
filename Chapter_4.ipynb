{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "# from tensorboardX import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ The \\ cross-entropy \\ method $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ Classes \\ and \\ Functions $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    \"\"\" custom Module that transforms the environment output into OneHot Encoder\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0,1.0, shape, dtype = np.float32) # In here we change the shape and output of the observation \n",
    "\n",
    "    def observation(self, observation):\n",
    "        res =  np.copy(self.observation_space.low)\n",
    "        res [observation] = 1.0\n",
    "        return res\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal mapper decision function for the agent to choose a \n",
    "    action to accumulate the higher possible rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # In here build the entire Neural Network \n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.FloatType):\n",
    "        return self.net(X)\n",
    "    \n",
    "\n",
    "# Batches Generator\n",
    "def iterate_batches(env:gym.make, Net: nn.Module, batch_size :int):\n",
    "    \"\"\" Iterate through batches and Run the Environments\"\"\"\n",
    "\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_step = []\n",
    "    obs = env.reset()[0] # This will take the Initial State from the environment \n",
    "    sm = nn.Softmax(dim=1)\n",
    " \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(np.array([obs])) # Transform the observation into Tensors in Torch\n",
    "        act_prob_v = sm(Net(obs_v)) # Calculate the probability at each iteration of the Neural Network\n",
    "        act_prob = act_prob_v.data.numpy()[0] # we need to unpack this by accessing the tensor.data field and then converting the tensor into a NumPy array\n",
    "        action = np.random.choice(len(act_prob), p = act_prob ) # look for why Len(act_prob)\n",
    "\n",
    "        next_obs, reward, is_done, _ , _= env.step(action) # it returns (observation, reward, terminated, truncated, info)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(observation=obs, action=action))\n",
    "\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps= episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs = env.reset()[0]\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "    \n",
    "def filter_batches (batch:namedtuple, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # Get those reward that are above the percentile \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step:step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ CartPole $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11109808\\AppData\\Local\\Temp\\ipykernel_25852\\1754641861.py:82: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  train_obs_v = torch.FloatTensor(train_obs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 0.6824373006820679, Reward Mean : 15.5625, Reward bound : 17.5\n",
      "iter_no ; 1, loss : 0.6878387331962585, Reward Mean : 19.3125, Reward bound : 18.5\n",
      "iter_no ; 2, loss : 0.6904373168945312, Reward Mean : 19.5, Reward bound : 23.5\n",
      "iter_no ; 3, loss : 0.6856494545936584, Reward Mean : 20.125, Reward bound : 26.5\n",
      "iter_no ; 4, loss : 0.646310031414032, Reward Mean : 23.6875, Reward bound : 27.5\n",
      "iter_no ; 5, loss : 0.702922523021698, Reward Mean : 16.375, Reward bound : 18.0\n",
      "iter_no ; 6, loss : 0.688866138458252, Reward Mean : 24.375, Reward bound : 28.5\n",
      "iter_no ; 7, loss : 0.6686075925827026, Reward Mean : 28.75, Reward bound : 27.0\n",
      "iter_no ; 8, loss : 0.6799336671829224, Reward Mean : 28.6875, Reward bound : 32.5\n",
      "iter_no ; 9, loss : 0.6793533563613892, Reward Mean : 33.6875, Reward bound : 29.5\n",
      "iter_no ; 10, loss : 0.6730258464813232, Reward Mean : 37.5625, Reward bound : 44.0\n",
      "iter_no ; 11, loss : 0.6673963069915771, Reward Mean : 34.3125, Reward bound : 36.5\n",
      "iter_no ; 12, loss : 0.6559596657752991, Reward Mean : 32.75, Reward bound : 34.0\n",
      "iter_no ; 13, loss : 0.6508161425590515, Reward Mean : 31.125, Reward bound : 35.0\n",
      "iter_no ; 14, loss : 0.6486320495605469, Reward Mean : 35.75, Reward bound : 34.5\n",
      "iter_no ; 15, loss : 0.6409075856208801, Reward Mean : 42.9375, Reward bound : 39.5\n",
      "iter_no ; 16, loss : 0.6424570679664612, Reward Mean : 55.6875, Reward bound : 78.5\n",
      "iter_no ; 17, loss : 0.6395326256752014, Reward Mean : 54.25, Reward bound : 54.5\n",
      "iter_no ; 18, loss : 0.646124541759491, Reward Mean : 56.75, Reward bound : 59.0\n",
      "iter_no ; 19, loss : 0.6313738822937012, Reward Mean : 53.4375, Reward bound : 57.5\n",
      "iter_no ; 20, loss : 0.6305128931999207, Reward Mean : 49.375, Reward bound : 45.5\n",
      "iter_no ; 21, loss : 0.6324881315231323, Reward Mean : 74.625, Reward bound : 83.5\n",
      "iter_no ; 22, loss : 0.6279325485229492, Reward Mean : 54.375, Reward bound : 57.5\n",
      "iter_no ; 23, loss : 0.6233900785446167, Reward Mean : 64.125, Reward bound : 80.5\n",
      "iter_no ; 24, loss : 0.6191789507865906, Reward Mean : 57.4375, Reward bound : 70.0\n",
      "iter_no ; 25, loss : 0.6118383407592773, Reward Mean : 71.375, Reward bound : 80.5\n",
      "iter_no ; 26, loss : 0.6071692109107971, Reward Mean : 83.375, Reward bound : 101.5\n",
      "iter_no ; 27, loss : 0.5984128713607788, Reward Mean : 84.8125, Reward bound : 98.5\n",
      "iter_no ; 28, loss : 0.5998115539550781, Reward Mean : 77.875, Reward bound : 85.0\n",
      "iter_no ; 29, loss : 0.5971394777297974, Reward Mean : 78.0625, Reward bound : 78.0\n",
      "iter_no ; 30, loss : 0.6045014262199402, Reward Mean : 69.0625, Reward bound : 76.0\n",
      "iter_no ; 31, loss : 0.6024150848388672, Reward Mean : 79.0, Reward bound : 98.5\n",
      "iter_no ; 32, loss : 0.5839741826057434, Reward Mean : 116.125, Reward bound : 124.0\n",
      "iter_no ; 33, loss : 0.5767242908477783, Reward Mean : 73.3125, Reward bound : 82.5\n",
      "iter_no ; 34, loss : 0.5935654640197754, Reward Mean : 80.125, Reward bound : 94.0\n",
      "iter_no ; 35, loss : 0.5906177759170532, Reward Mean : 87.5, Reward bound : 107.0\n",
      "iter_no ; 36, loss : 0.5814305543899536, Reward Mean : 115.625, Reward bound : 133.5\n",
      "iter_no ; 37, loss : 0.5947632193565369, Reward Mean : 101.625, Reward bound : 124.5\n",
      "iter_no ; 38, loss : 0.5854528546333313, Reward Mean : 115.0, Reward bound : 136.5\n",
      "iter_no ; 39, loss : 0.5798904895782471, Reward Mean : 118.9375, Reward bound : 124.5\n",
      "iter_no ; 40, loss : 0.583992063999176, Reward Mean : 144.25, Reward bound : 158.5\n",
      "iter_no ; 41, loss : 0.5743201971054077, Reward Mean : 146.5, Reward bound : 177.0\n",
      "iter_no ; 42, loss : 0.5851582288742065, Reward Mean : 168.1875, Reward bound : 197.5\n",
      "iter_no ; 43, loss : 0.5840165019035339, Reward Mean : 210.5625, Reward bound : 205.5\n",
      "solved!\n"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network \n",
    "HIDDEN_LAYERS = 50\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiating Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size_ = env.observation_space.shape[0] # number of output in the environment ->  ndarray with shape (1,) which takes values {0,1} where 0, push cart to the left, and 1, push cart to the right  \n",
    "n_actions_ = env.action_space.n  #left , Right\n",
    "\n",
    "# Initiate Neural Network , Loss Functions and Optimizer\n",
    "net = NeuralNetwork(obs_size = obs_size_, hidden_size= HIDDEN_LAYERS, n_actions = n_actions_)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.01)\n",
    "# writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Net=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation\n",
    "    # if reward_b > 400:\n",
    "        # time.sleep(0.02) \n",
    "        # env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    # writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    # writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    # writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 200:\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "    # writer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ Frozen \\ Lake \\ - \\ Naive$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 1.3898205757141113, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 1, loss : 1.3752325773239136, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 2, loss : 1.3727778196334839, Reward Mean : 0.125, Reward bound : 0.0\n",
      "iter_no ; 3, loss : 1.3739104270935059, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 4, loss : 1.3824213743209839, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 5, loss : 1.3741878271102905, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 6, loss : 1.3988438844680786, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 7, loss : 1.3869667053222656, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 8, loss : 1.3656502962112427, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 9, loss : 1.3733731508255005, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 10, loss : 1.3804473876953125, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 11, loss : 1.358634114265442, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 12, loss : 1.365053415298462, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 13, loss : 1.3937311172485352, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 14, loss : 1.3860588073730469, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 15, loss : 1.378901481628418, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 16, loss : 1.3873685598373413, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 17, loss : 1.417508840560913, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 18, loss : 1.3545185327529907, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 19, loss : 1.363614797592163, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 20, loss : 1.3598833084106445, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 21, loss : 1.367145299911499, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 22, loss : 1.387797236442566, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 23, loss : 1.3679581880569458, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 24, loss : 1.3752923011779785, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 25, loss : 1.3585211038589478, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 26, loss : 1.387014389038086, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 27, loss : 1.3530943393707275, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 28, loss : 1.392802357673645, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 29, loss : 1.3409130573272705, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 30, loss : 1.3766505718231201, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 31, loss : 1.373561143875122, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 32, loss : 1.3827605247497559, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 33, loss : 1.3596974611282349, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 34, loss : 1.398086667060852, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 35, loss : 1.3624119758605957, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 36, loss : 1.3945057392120361, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 37, loss : 1.3547189235687256, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 38, loss : 1.3511499166488647, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 39, loss : 1.3849612474441528, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 40, loss : 1.349585771560669, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 41, loss : 1.341360092163086, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 42, loss : 1.3619526624679565, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 43, loss : 1.3700083494186401, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 44, loss : 1.3647350072860718, Reward Mean : 0.125, Reward bound : 0.0\n",
      "iter_no ; 45, loss : 1.4006929397583008, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 46, loss : 1.3926547765731812, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 47, loss : 1.3883838653564453, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 48, loss : 1.373966097831726, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 49, loss : 1.3704131841659546, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 50, loss : 1.3853586912155151, Reward Mean : 0.0, Reward bound : 0.0\n",
      "solved!\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiate Environment \n",
    "env = DiscreteOneHotWrapper(gym.make('FrozenLake-v1')) # In here we change the Initiate State actions\n",
    "obs_size_ = env.observation_space.shape[0] # shape [16, ]\n",
    "n_actions_ = env.action_space.n # array from 0 to 4 where each number equal to an action UP DOWN LEFT RIGHT\n",
    "\n",
    "# Initiate Neural Network\n",
    "net = NeuralNetwork(obs_size=obs_size_, hidden_size=HIDDEN_SIZE, n_actions=n_actions_) # obs_size =[16, ],HIDDEN_SIZE = 128 , n_actions = np.array[0,4]\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr = 0.001)\n",
    "writer = SummaryWriter(comment='-frozenlake-naive')\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Net=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation\n",
    "    # if reward_b > 400:\n",
    "        # time.sleep(0.02) \n",
    "        # env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 0.8 or iter_no == 50:\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ Frozen \\ Lake \\ - \\ Tweaked $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batches_and_gamma (batch:namedtuple, gamma_penalty:float, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN using and Penalty GAMMA\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - gamma_penalty: float, percentage to transform the elite into a percentage instead of 1\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    filter_function = lambda s:s.reward * (gamma_penalty ** len(s.steps))\n",
    "    discounted_reward = list(map(filter_function, batch))\n",
    "    reward_bound = np.percentile(discounted_reward, percentile) # Get those reward that are above the percentile with the gamma_penalty applied\n",
    "  \n",
    "    train_obs, train_act, elite_batch  = [], [] , []\n",
    "    for example, discounted_rw in zip(batch, discounted_reward):\n",
    "        if discounted_rw > reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step:step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    # train_obs_v = torch.FloatTensor(train_obs)\n",
    "    # train_act_v = torch.LongTensor(train_act)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 1.3785868883132935, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 1, loss : 1.378678560256958, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 2, loss : 1.3657974004745483, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 3, loss : 1.360249400138855, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 4, loss : 1.3615710735321045, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 5, loss : 1.357996940612793, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 6, loss : 1.3532918691635132, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 7, loss : 1.3488185405731201, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 8, loss : 1.3447781801223755, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 9, loss : 1.3390018939971924, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 10, loss : 1.339638113975525, Reward Mean : 0.05, Reward bound : 0.0\n",
      "iter_no ; 11, loss : 1.3393833637237549, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 12, loss : 1.3383861780166626, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 13, loss : 1.339589238166809, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 14, loss : 1.3316097259521484, Reward Mean : 0.06, Reward bound : 0.0\n",
      "iter_no ; 15, loss : 1.3289963006973267, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 16, loss : 1.3227099180221558, Reward Mean : 0.05, Reward bound : 0.0\n",
      "iter_no ; 17, loss : 1.3188107013702393, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 18, loss : 1.3164860010147095, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 19, loss : 1.313004970550537, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 20, loss : 1.307131052017212, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 21, loss : 1.3038175106048584, Reward Mean : 0.04, Reward bound : 0.0\n",
      "iter_no ; 22, loss : 1.2994359731674194, Reward Mean : 0.01, Reward bound : 0.0\n",
      "iter_no ; 23, loss : 1.2964335680007935, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 24, loss : 1.291573405265808, Reward Mean : 0.05, Reward bound : 0.0\n",
      "iter_no ; 25, loss : 1.2874113321304321, Reward Mean : 0.02, Reward bound : 0.0\n",
      "iter_no ; 26, loss : 1.282720923423767, Reward Mean : 0.05, Reward bound : 0.0\n",
      "iter_no ; 27, loss : 1.2809233665466309, Reward Mean : 0.06, Reward bound : 0.0\n",
      "iter_no ; 28, loss : 1.277796983718872, Reward Mean : 0.04, Reward bound : 0.0\n",
      "iter_no ; 29, loss : 1.2736371755599976, Reward Mean : 0.07, Reward bound : 0.0\n",
      "iter_no ; 30, loss : 1.2686690092086792, Reward Mean : 0.05, Reward bound : 0.0\n",
      "iter_no ; 31, loss : 1.2641627788543701, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 32, loss : 1.261096715927124, Reward Mean : 0.07, Reward bound : 0.0\n",
      "iter_no ; 33, loss : 1.2559624910354614, Reward Mean : 0.04, Reward bound : 0.0\n",
      "iter_no ; 34, loss : 1.2476763725280762, Reward Mean : 0.12, Reward bound : 0.0\n",
      "iter_no ; 35, loss : 1.2422547340393066, Reward Mean : 0.09, Reward bound : 0.0\n",
      "iter_no ; 36, loss : 1.2386298179626465, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 37, loss : 1.231860637664795, Reward Mean : 0.17, Reward bound : 0.0\n",
      "iter_no ; 38, loss : 1.2275094985961914, Reward Mean : 0.03, Reward bound : 0.0\n",
      "iter_no ; 39, loss : 1.2224494218826294, Reward Mean : 0.08, Reward bound : 0.0\n",
      "iter_no ; 40, loss : 1.2208771705627441, Reward Mean : 0.07, Reward bound : 0.0\n",
      "iter_no ; 41, loss : 1.2175772190093994, Reward Mean : 0.1, Reward bound : 0.0\n",
      "iter_no ; 42, loss : 1.2149615287780762, Reward Mean : 0.07, Reward bound : 0.0\n",
      "iter_no ; 43, loss : 1.2106558084487915, Reward Mean : 0.06, Reward bound : 0.0\n",
      "iter_no ; 44, loss : 1.206926703453064, Reward Mean : 0.11, Reward bound : 0.0\n",
      "iter_no ; 45, loss : 1.2043184041976929, Reward Mean : 0.09, Reward bound : 0.0\n",
      "iter_no ; 46, loss : 1.1968363523483276, Reward Mean : 0.09, Reward bound : 0.06011677748931561\n",
      "iter_no ; 47, loss : 1.188674807548523, Reward Mean : 0.07, Reward bound : 0.07577812092302894\n",
      "iter_no ; 48, loss : 1.1738059520721436, Reward Mean : 0.1, Reward bound : 0.13508517176729928\n",
      "iter_no ; 49, loss : 1.1620882749557495, Reward Mean : 0.15, Reward bound : 0.16677181699666577\n",
      "iter_no ; 50, loss : 1.1433767080307007, Reward Mean : 0.15, Reward bound : 0.20589113209464907\n",
      "iter_no ; 51, loss : 1.123928427696228, Reward Mean : 0.13, Reward bound : 0.2287679245496101\n",
      "iter_no ; 52, loss : 1.1053056716918945, Reward Mean : 0.21, Reward bound : 0.2541865828329001\n",
      "iter_no ; 53, loss : 1.073486328125, Reward Mean : 0.15, Reward bound : 0.2824295364810001\n",
      "iter_no ; 54, loss : 1.0714823007583618, Reward Mean : 0.14, Reward bound : 0.07878167217468866\n",
      "iter_no ; 55, loss : 1.0635358095169067, Reward Mean : 0.15, Reward bound : 0.18530201888518416\n",
      "iter_no ; 56, loss : 1.0502779483795166, Reward Mean : 0.15, Reward bound : 0.2824295364810001\n",
      "iter_no ; 57, loss : 1.0159341096878052, Reward Mean : 0.21, Reward bound : 0.31381059609000006\n",
      "iter_no ; 58, loss : 0.981229841709137, Reward Mean : 0.2, Reward bound : 0.3486784401000001\n",
      "iter_no ; 59, loss : 0.9949898719787598, Reward Mean : 0.18, Reward bound : 0.0\n",
      "iter_no ; 60, loss : 0.9773609638214111, Reward Mean : 0.22, Reward bound : 0.271697214094722\n",
      "iter_no ; 61, loss : 0.9549949765205383, Reward Mean : 0.22, Reward bound : 0.3486784401000001\n",
      "iter_no ; 62, loss : 0.889390766620636, Reward Mean : 0.2, Reward bound : 0.3874204890000001\n",
      "iter_no ; 63, loss : 0.9143412709236145, Reward Mean : 0.24, Reward bound : 0.0\n",
      "iter_no ; 64, loss : 0.9096267223358154, Reward Mean : 0.2, Reward bound : 0.1894198415270772\n",
      "iter_no ; 65, loss : 0.8815261125564575, Reward Mean : 0.25, Reward bound : 0.2824295364810001\n",
      "iter_no ; 66, loss : 0.8547385334968567, Reward Mean : 0.24, Reward bound : 0.3486784401000001\n",
      "iter_no ; 67, loss : 0.836981475353241, Reward Mean : 0.26, Reward bound : 0.3874204890000001\n",
      "iter_no ; 68, loss : 0.7735826373100281, Reward Mean : 0.23, Reward bound : 0.4304672100000001\n",
      "iter_no ; 69, loss : 0.7964377999305725, Reward Mean : 0.21, Reward bound : 0.0\n",
      "iter_no ; 70, loss : 0.779449462890625, Reward Mean : 0.3, Reward bound : 0.31381059609000006\n",
      "iter_no ; 71, loss : 0.761701762676239, Reward Mean : 0.32, Reward bound : 0.3486784401000001\n",
      "iter_no ; 72, loss : 0.7262997031211853, Reward Mean : 0.34, Reward bound : 0.4304672100000001\n",
      "iter_no ; 73, loss : 0.7152801752090454, Reward Mean : 0.28, Reward bound : 0.4304672100000001\n",
      "iter_no ; 74, loss : 0.6246214509010315, Reward Mean : 0.23, Reward bound : 0.4782969000000001\n",
      "iter_no ; 75, loss : 0.7063618302345276, Reward Mean : 0.36, Reward bound : 0.1791710385072548\n",
      "iter_no ; 76, loss : 0.6802477836608887, Reward Mean : 0.35, Reward bound : 0.3486784401000001\n",
      "iter_no ; 77, loss : 0.649065375328064, Reward Mean : 0.37, Reward bound : 0.3874204890000001\n",
      "iter_no ; 78, loss : 0.6451569199562073, Reward Mean : 0.28, Reward bound : 0.3874204890000001\n",
      "iter_no ; 79, loss : 0.5921006202697754, Reward Mean : 0.39, Reward bound : 0.4304672100000001\n",
      "iter_no ; 80, loss : 0.5479227304458618, Reward Mean : 0.36, Reward bound : 0.4782969000000001\n",
      "iter_no ; 81, loss : 0.534867525100708, Reward Mean : 0.4, Reward bound : 0.4782969000000001\n",
      "iter_no ; 82, loss : 0.5225192904472351, Reward Mean : 0.4, Reward bound : 0.4782969000000001\n",
      "iter_no ; 84, loss : 0.8488152027130127, Reward Mean : 0.4, Reward bound : 0.0\n",
      "iter_no ; 85, loss : 0.8067300319671631, Reward Mean : 0.38, Reward bound : 0.0\n",
      "iter_no ; 86, loss : 0.7677235007286072, Reward Mean : 0.34, Reward bound : 0.0\n",
      "iter_no ; 87, loss : 0.7471219897270203, Reward Mean : 0.39, Reward bound : 0.26265946892733\n",
      "iter_no ; 88, loss : 0.6634029746055603, Reward Mean : 0.4, Reward bound : 0.3486784401000001\n",
      "iter_no ; 89, loss : 0.5583168864250183, Reward Mean : 0.45, Reward bound : 0.4304672100000001\n",
      "iter_no ; 90, loss : 0.5487557649612427, Reward Mean : 0.41, Reward bound : 0.45438205500000006\n",
      "iter_no ; 91, loss : 0.44699591398239136, Reward Mean : 0.54, Reward bound : 0.4782969000000001\n",
      "iter_no ; 92, loss : 0.44002288579940796, Reward Mean : 0.51, Reward bound : 0.4782969000000001\n",
      "iter_no ; 94, loss : 0.6335299611091614, Reward Mean : 0.46, Reward bound : 0.0\n",
      "iter_no ; 95, loss : 0.6519396901130676, Reward Mean : 0.56, Reward bound : 0.12709329141645004\n",
      "iter_no ; 96, loss : 0.5425765514373779, Reward Mean : 0.54, Reward bound : 0.3874204890000001\n",
      "iter_no ; 97, loss : 0.49051418900489807, Reward Mean : 0.54, Reward bound : 0.4304672100000001\n",
      "iter_no ; 98, loss : 0.40881437063217163, Reward Mean : 0.53, Reward bound : 0.4782969000000001\n",
      "iter_no ; 99, loss : 0.40027403831481934, Reward Mean : 0.54, Reward bound : 0.4782969000000001\n",
      "iter_no ; 101, loss : 0.5924339294433594, Reward Mean : 0.6, Reward bound : 0.0\n",
      "iter_no ; 102, loss : 0.5280975103378296, Reward Mean : 0.61, Reward bound : 0.3874204890000001\n",
      "iter_no ; 103, loss : 0.4669576585292816, Reward Mean : 0.63, Reward bound : 0.4304672100000001\n",
      "iter_no ; 104, loss : 0.3739090859889984, Reward Mean : 0.69, Reward bound : 0.4782969000000001\n",
      "iter_no ; 105, loss : 0.36892566084861755, Reward Mean : 0.53, Reward bound : 0.4782969000000001\n",
      "iter_no ; 107, loss : 0.5739400386810303, Reward Mean : 0.66, Reward bound : 0.0\n",
      "iter_no ; 108, loss : 0.5343248844146729, Reward Mean : 0.61, Reward bound : 0.3874204890000001\n",
      "iter_no ; 109, loss : 0.4551280438899994, Reward Mean : 0.67, Reward bound : 0.4304672100000001\n",
      "iter_no ; 110, loss : 0.3471944034099579, Reward Mean : 0.68, Reward bound : 0.4782969000000001\n",
      "iter_no ; 112, loss : 0.5149679780006409, Reward Mean : 0.68, Reward bound : 0.0\n",
      "iter_no ; 113, loss : 0.4141599237918854, Reward Mean : 0.65, Reward bound : 0.4304672100000001\n",
      "iter_no ; 114, loss : 0.3356758654117584, Reward Mean : 0.75, Reward bound : 0.4782969000000001\n",
      "iter_no ; 116, loss : 0.4773103594779968, Reward Mean : 0.67, Reward bound : 0.0\n",
      "iter_no ; 117, loss : 0.3964972198009491, Reward Mean : 0.75, Reward bound : 0.4304672100000001\n",
      "iter_no ; 118, loss : 0.3092919588088989, Reward Mean : 0.71, Reward bound : 0.4782969000000001\n",
      "iter_no ; 120, loss : 0.43774640560150146, Reward Mean : 0.66, Reward bound : 0.0\n",
      "iter_no ; 121, loss : 0.36775097250938416, Reward Mean : 0.69, Reward bound : 0.45438205500000006\n",
      "iter_no ; 122, loss : 0.2958358824253082, Reward Mean : 0.68, Reward bound : 0.4782969000000001\n",
      "iter_no ; 124, loss : 0.3693906366825104, Reward Mean : 0.78, Reward bound : 0.4304672100000001\n",
      "iter_no ; 125, loss : 0.29813864827156067, Reward Mean : 0.74, Reward bound : 0.4782969000000001\n",
      "iter_no ; 127, loss : 0.42769715189933777, Reward Mean : 0.68, Reward bound : 0.0\n",
      "iter_no ; 128, loss : 0.2807618975639343, Reward Mean : 0.67, Reward bound : 0.4782969000000001\n",
      "iter_no ; 130, loss : 0.3606746196746826, Reward Mean : 0.75, Reward bound : 0.4304672100000001\n",
      "iter_no ; 131, loss : 0.28218692541122437, Reward Mean : 0.73, Reward bound : 0.4782969000000001\n",
      "iter_no ; 133, loss : 0.31855547428131104, Reward Mean : 0.76, Reward bound : 0.4304672100000001\n",
      "iter_no ; 134, loss : 0.2582033574581146, Reward Mean : 0.81, Reward bound : 0.52612659\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# Hard core Variables\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "random.seed(104)\n",
    "\n",
    "# Keeping Records of Batches\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "# Initiate the Environment\n",
    "env = DiscreteOneHotWrapper(gym.make('FrozenLake-v1', is_slippery = False))#,render_mode = 'human'))\n",
    "obs_size_ = env.observation_space.shape[0]\n",
    "n_actions_ = env.action_space.n\n",
    "\n",
    "# Initiate Neural Network\n",
    "net= NeuralNetwork(obs_size=obs_size_, hidden_size=HIDDEN_SIZE, n_actions=n_actions_)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr = 0.001)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-tweaked-no-slippery\")\n",
    "\n",
    "full_batch = []\n",
    "for iter_no , batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_mean = float(np.mean(list(map(lambda s:s.reward, batch))))\n",
    "    full_batch, obs, acts, reward_bound = filter_batches_and_gamma(batch = full_batch + batch, gamma_penalty=GAMMA, percentile=PERCENTILE)\n",
    "    if not full_batch:\n",
    "        continue\n",
    "    obs_v = torch.FloatTensor(obs)\n",
    "    act_v = torch.LongTensor(acts)\n",
    "    full_batch = full_batch[-500:]\n",
    "\n",
    "    # Optimizing and Updating Neural Network\n",
    "    optimizer.zero_grad()\n",
    "    action_score_v = net(obs_v)\n",
    "    loss_v = objective(action_score_v, act_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_mean}, Reward bound : {reward_bound}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "\n",
    "    if reward_mean > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        env.close()\n",
    "        break\n",
    "    \n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
