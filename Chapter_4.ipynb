{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ The \\ cross-entropy \\ method $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ CartPole $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal mapper decision function for the agent to choose a \n",
    "    action to accumulate the higher possible rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # In here build the entire Neural Network \n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.FloatType):\n",
    "        return self.net(X)\n",
    "    \n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "\n",
    "# Batches Generator\n",
    "def iterate_batches(env:gym.make, Net: nn.Module, batch_size :int):\n",
    "    \"\"\" Iterate through batches and Run the Environments\"\"\"\n",
    "\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_step = []\n",
    "    obs = env.reset()[0] # This will take the Initial State from the environment \n",
    "    sm = nn.Softmax(dim=1)\n",
    " \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(np.array([obs])) # Transform the observation into Tensors in Torch\n",
    "        act_prob_v = sm(Net(obs_v)) # Calculate the probability at each iteration of the Neural Network\n",
    "        act_prob = act_prob_v.data.numpy()[0] # we need to unpack this by accessing the tensor.data field and then converting the tensor into a NumPy array\n",
    "        action = np.random.choice(len(act_prob), p = act_prob ) # look for why Len(act_prob)\n",
    "\n",
    "        next_obs, reward, is_done, _ , _= env.step(action) # it returns (observation, reward, terminated, truncated, info)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(observation=obs, action=action))\n",
    "\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps= episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs = env.reset()[0]\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "    \n",
    "def filter_batches (batch:namedtuple, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # Get those reward that are above the percentile \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step:step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 0.6806756258010864, Reward Mean : 19.8125, Reward bound : 21.0\n",
      "iter_no ; 1, loss : 0.6808675527572632, Reward Mean : 23.875, Reward bound : 28.5\n",
      "iter_no ; 2, loss : 0.6774863600730896, Reward Mean : 29.625, Reward bound : 34.5\n",
      "iter_no ; 3, loss : 0.6752472519874573, Reward Mean : 25.375, Reward bound : 30.0\n",
      "iter_no ; 4, loss : 0.6570502519607544, Reward Mean : 22.5625, Reward bound : 23.5\n",
      "iter_no ; 5, loss : 0.6543666124343872, Reward Mean : 39.625, Reward bound : 52.0\n",
      "iter_no ; 6, loss : 0.651344358921051, Reward Mean : 30.8125, Reward bound : 29.5\n",
      "iter_no ; 7, loss : 0.6459672451019287, Reward Mean : 35.75, Reward bound : 39.0\n",
      "iter_no ; 8, loss : 0.632156491279602, Reward Mean : 42.25, Reward bound : 47.5\n",
      "iter_no ; 9, loss : 0.6263872385025024, Reward Mean : 39.3125, Reward bound : 42.5\n",
      "iter_no ; 10, loss : 0.6256864070892334, Reward Mean : 40.3125, Reward bound : 42.5\n",
      "iter_no ; 11, loss : 0.6307383179664612, Reward Mean : 49.8125, Reward bound : 60.5\n",
      "iter_no ; 12, loss : 0.6035966873168945, Reward Mean : 45.625, Reward bound : 51.0\n",
      "iter_no ; 13, loss : 0.6098659634590149, Reward Mean : 50.0625, Reward bound : 49.0\n",
      "iter_no ; 14, loss : 0.6143519878387451, Reward Mean : 57.5, Reward bound : 67.5\n",
      "iter_no ; 15, loss : 0.6069211959838867, Reward Mean : 51.75, Reward bound : 55.0\n",
      "iter_no ; 16, loss : 0.6019743084907532, Reward Mean : 60.1875, Reward bound : 70.0\n",
      "iter_no ; 17, loss : 0.5923683643341064, Reward Mean : 49.0625, Reward bound : 58.0\n",
      "iter_no ; 18, loss : 0.5886812806129456, Reward Mean : 55.75, Reward bound : 66.5\n",
      "iter_no ; 19, loss : 0.580018937587738, Reward Mean : 53.375, Reward bound : 53.0\n",
      "iter_no ; 20, loss : 0.5716745257377625, Reward Mean : 69.1875, Reward bound : 78.0\n",
      "iter_no ; 21, loss : 0.5944546461105347, Reward Mean : 45.75, Reward bound : 55.5\n",
      "iter_no ; 22, loss : 0.5579139590263367, Reward Mean : 61.625, Reward bound : 66.0\n",
      "iter_no ; 23, loss : 0.5657393336296082, Reward Mean : 70.375, Reward bound : 84.5\n",
      "iter_no ; 24, loss : 0.5624542832374573, Reward Mean : 70.6875, Reward bound : 81.5\n",
      "iter_no ; 25, loss : 0.5584930181503296, Reward Mean : 57.8125, Reward bound : 55.5\n",
      "iter_no ; 26, loss : 0.5737038850784302, Reward Mean : 62.875, Reward bound : 73.5\n",
      "iter_no ; 27, loss : 0.562001645565033, Reward Mean : 62.75, Reward bound : 70.0\n",
      "iter_no ; 28, loss : 0.5253898501396179, Reward Mean : 62.5, Reward bound : 75.0\n",
      "iter_no ; 29, loss : 0.5518019199371338, Reward Mean : 68.0, Reward bound : 71.0\n",
      "iter_no ; 30, loss : 0.5341655611991882, Reward Mean : 69.4375, Reward bound : 76.5\n",
      "iter_no ; 31, loss : 0.5640508532524109, Reward Mean : 67.5, Reward bound : 66.5\n",
      "iter_no ; 32, loss : 0.5489962697029114, Reward Mean : 71.125, Reward bound : 78.0\n",
      "iter_no ; 33, loss : 0.5543223023414612, Reward Mean : 77.8125, Reward bound : 77.0\n",
      "iter_no ; 34, loss : 0.515716552734375, Reward Mean : 77.375, Reward bound : 92.5\n",
      "iter_no ; 35, loss : 0.5450964570045471, Reward Mean : 91.75, Reward bound : 104.0\n",
      "iter_no ; 36, loss : 0.5368618965148926, Reward Mean : 87.125, Reward bound : 106.0\n",
      "iter_no ; 37, loss : 0.5488793253898621, Reward Mean : 109.5, Reward bound : 115.0\n",
      "iter_no ; 38, loss : 0.5273402333259583, Reward Mean : 94.3125, Reward bound : 102.5\n",
      "iter_no ; 39, loss : 0.5636366009712219, Reward Mean : 87.9375, Reward bound : 96.5\n",
      "iter_no ; 40, loss : 0.5501959323883057, Reward Mean : 99.875, Reward bound : 115.0\n",
      "iter_no ; 41, loss : 0.550033688545227, Reward Mean : 105.375, Reward bound : 122.5\n",
      "iter_no ; 42, loss : 0.5450732111930847, Reward Mean : 135.625, Reward bound : 138.5\n",
      "iter_no ; 43, loss : 0.5388917326927185, Reward Mean : 117.8125, Reward bound : 131.0\n",
      "iter_no ; 44, loss : 0.5478055477142334, Reward Mean : 141.0, Reward bound : 151.0\n",
      "iter_no ; 45, loss : 0.5532531142234802, Reward Mean : 158.8125, Reward bound : 185.5\n",
      "iter_no ; 46, loss : 0.5309880375862122, Reward Mean : 150.25, Reward bound : 151.5\n",
      "iter_no ; 47, loss : 0.5419881343841553, Reward Mean : 135.875, Reward bound : 157.0\n",
      "iter_no ; 48, loss : 0.5369976162910461, Reward Mean : 160.625, Reward bound : 163.0\n",
      "iter_no ; 49, loss : 0.543327271938324, Reward Mean : 168.0625, Reward bound : 166.0\n",
      "iter_no ; 50, loss : 0.5377697944641113, Reward Mean : 159.5, Reward bound : 202.0\n",
      "iter_no ; 51, loss : 0.5527777075767517, Reward Mean : 163.625, Reward bound : 187.0\n",
      "iter_no ; 52, loss : 0.5334165692329407, Reward Mean : 192.8125, Reward bound : 203.5\n",
      "iter_no ; 53, loss : 0.542898952960968, Reward Mean : 170.5, Reward bound : 195.5\n",
      "iter_no ; 54, loss : 0.5441837906837463, Reward Mean : 186.25, Reward bound : 211.5\n",
      "iter_no ; 55, loss : 0.5312480330467224, Reward Mean : 201.5, Reward bound : 229.5\n",
      "solved!\n"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network \n",
    "HIDDEN_LAYERS = 50\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Initiating Environment\n",
    "env = gym.make(\"CartPole-v1\") #, render_mode = 'human')\n",
    "obs_size_ = env.observation_space.shape[0] # number of output in the environment ->  ndarray with shape (1,) which takes values {0,1} where 0, push cart to the left, and 1, push cart to the right  \n",
    "n_actions_ = env.action_space.n  #left , Right\n",
    "\n",
    "# Initiate Neural Network , Loss Functions and Optimizer\n",
    "net = NeuralNetwork(obs_size = obs_size_, hidden_size= HIDDEN_LAYERS, n_actions = n_actions_)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Net=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation\n",
    "    # if reward_b > 400:\n",
    "        # time.sleep(0.02) \n",
    "        # env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 200:\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "    writer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ Frozen \\ Lake $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    \"\"\" custom Module that transforms the environment output into OneHot Encoder\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0,1.0, shape, dtype = np.float32) # In here we change the shape and output of the observation \n",
    "\n",
    "    def observation(self, observation):\n",
    "        res =  np.copy(self.observation_space.low)\n",
    "        res [observation] = 1.0\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 1.3898205757141113, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 1, loss : 1.3752325773239136, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 2, loss : 1.3727778196334839, Reward Mean : 0.125, Reward bound : 0.0\n",
      "iter_no ; 3, loss : 1.3739104270935059, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 4, loss : 1.3824213743209839, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 5, loss : 1.3741878271102905, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 6, loss : 1.3988438844680786, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 7, loss : 1.3869667053222656, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 8, loss : 1.3656502962112427, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 9, loss : 1.3733731508255005, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 10, loss : 1.3804473876953125, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 11, loss : 1.358634114265442, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 12, loss : 1.365053415298462, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 13, loss : 1.3937311172485352, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 14, loss : 1.3860588073730469, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 15, loss : 1.378901481628418, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 16, loss : 1.3873685598373413, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 17, loss : 1.417508840560913, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 18, loss : 1.3545185327529907, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 19, loss : 1.363614797592163, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 20, loss : 1.3598833084106445, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 21, loss : 1.367145299911499, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 22, loss : 1.387797236442566, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 23, loss : 1.3679581880569458, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 24, loss : 1.3752923011779785, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 25, loss : 1.3585211038589478, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 26, loss : 1.387014389038086, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 27, loss : 1.3530943393707275, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 28, loss : 1.392802357673645, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 29, loss : 1.3409130573272705, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 30, loss : 1.3766505718231201, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 31, loss : 1.373561143875122, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 32, loss : 1.3827605247497559, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 33, loss : 1.3596974611282349, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 34, loss : 1.398086667060852, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 35, loss : 1.3624119758605957, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 36, loss : 1.3945057392120361, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 37, loss : 1.3547189235687256, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 38, loss : 1.3511499166488647, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 39, loss : 1.3849612474441528, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 40, loss : 1.349585771560669, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 41, loss : 1.341360092163086, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 42, loss : 1.3619526624679565, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 43, loss : 1.3700083494186401, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 44, loss : 1.3647350072860718, Reward Mean : 0.125, Reward bound : 0.0\n",
      "iter_no ; 45, loss : 1.4006929397583008, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 46, loss : 1.3926547765731812, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 47, loss : 1.3883838653564453, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 48, loss : 1.373966097831726, Reward Mean : 0.0625, Reward bound : 0.0\n",
      "iter_no ; 49, loss : 1.3704131841659546, Reward Mean : 0.0, Reward bound : 0.0\n",
      "iter_no ; 50, loss : 1.3853586912155151, Reward Mean : 0.0, Reward bound : 0.0\n",
      "solved!\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiate Environment \n",
    "env = DiscreteOneHotWrapper(gym.make('FrozenLake-v1')) # In here we change the Initiate State actions\n",
    "obs_size_ = env.observation_space.shape[0] # shape [16, ]\n",
    "n_actions_ = env.action_space.n # array from 0 to 4 where each number equal to an action UP DOWN LEFT RIGHT\n",
    "\n",
    "# Initiate Neural Network\n",
    "net = NeuralNetwork(obs_size=obs_size_, hidden_size=HIDDEN_SIZE, n_actions=n_actions_) # obs_size =[16, ],HIDDEN_SIZE = 128 , n_actions = np.array[0,4]\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr = 0.001)\n",
    "writer = SummaryWriter(comment='-frozenlake-naive')\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Net=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation\n",
    "    # if reward_b > 400:\n",
    "        # time.sleep(0.02) \n",
    "        # env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 0.8 or iter_no == 50:\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
