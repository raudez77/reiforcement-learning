{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ The \\ cross-entropy \\ method \\ on \\ CartPole $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNet(nn.Module):\n",
    "    \"\"\" Internal mapper decision function for the agent to choose a \n",
    "    action to accumulate the higher possible rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(InnerNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # In here build the entire Neural Network \n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.FloatType):\n",
    "        return self.net(X)\n",
    "    \n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "\n",
    "# Batches Generator\n",
    "def iterate_batches(env:gym.make, Net: nn.Module, batch_size :int):\n",
    "    \"\"\" Iterate through batches and Run the Environments\"\"\"\n",
    "\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_step = []\n",
    "    obs = env.reset()  # This will take the Initial State from the environment \n",
    "    print(obs.shape)\n",
    "    sm = nn.Softmax(dim=1)\n",
    " \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # Transform the observation into Tensors in Torch\n",
    "        act_prob_v = sm(Net(obs_v)) # Calculate the probability at each iteration of the Neural Network\n",
    "        act_prob = act_prob_v.data.numpy()[0] # we need to unpack this by accessing the tensor.data field and then converting the tensor into a NumPy array\n",
    "        action = np.random.choice(len(act_prob), p = act_prob ) # look for why Len(act_prob)\n",
    "\n",
    "        next_obs, reward, is_done, _ , _= env.step(action) # it returns (observation, reward, terminated, truncated, info)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(observation=obs, action=action))\n",
    "\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps= episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "    \n",
    "def filter_batches (batch:namedtuple, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.rewards, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # Get those reward that are above the percentile \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step:step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\reiforcement-learning\\Chapter_4.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(comment\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-cartpole\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Start Training \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m iter_no, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(iterate_batches(env\u001b[39m=\u001b[39menv, Net\u001b[39m=\u001b[39mnet, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Applying optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     obs_v, acts_v, reward_b, reward_m \u001b[39m=\u001b[39m filter_batches(batch\u001b[39m=\u001b[39mbatch, percentile\u001b[39m=\u001b[39mPERCENTILE) \u001b[39m# Take the best Scenarios\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Reset the Gradient \u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\reiforcement-learning\\Chapter_4.ipynb Cell 4\u001b[0m in \u001b[0;36miterate_batches\u001b[1;34m(env, Net, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m sm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     obs_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor([obs]) \u001b[39m# Transform the observation into Tensors in Torch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     act_prob_v \u001b[39m=\u001b[39m sm(Net(obs_v)) \u001b[39m# Calculate the probability at each iteration of the Neural Network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/reiforcement-learning/Chapter_4.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     act_prob \u001b[39m=\u001b[39m act_prob_v\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m] \u001b[39m# we need to unpack this by accessing the tensor.data field and then converting the tensor into a NumPy array\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network \n",
    "HIDDEN_LAYERS = 50\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Initiating Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size_ = env.observation_space.shape[0] # number of output in the environment ->  ndarray with shape (1,) which takes values {0,1} where 0, push cart to the left, and 1, push cart to the right  \n",
    "n_actions_ = env.action_space.n  #left , Right\n",
    "\n",
    "# Initiate Neural Network , Loss Functions and Optimizer\n",
    "net = InnerNet(obs_size = obs_size_, hidden_size= HIDDEN_LAYERS, n_actions = n_actions_)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Net=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation \n",
    "\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 199:\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0271,  0.0373, -0.0096,  0.0248])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(np.array([ 0.02712901,  0.03728559, -0.00959169,  0.02476843]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
