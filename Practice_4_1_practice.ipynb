{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Functions \\ and \\ Classes $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal NN Policy mapper decision for the agent to choose an action \"\"\"\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(PolicyNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, X:torch.FloatTensor)-> torch.FloatTensor:\n",
    "        return torch.tanh(self.policy(X))*2 # Tanh activation maps the output to the range (-1, 1) , # Scale the output to the range (-2, 2)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ the \\ Pendulum \\ Problem \\ with \\ Cross \\ Entropy $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env:gym.make, Policy:nn.Module, batch_size:int, max_episode_steps:int=20):\n",
    "    \"\"\" Iterate thorugh batches and run the enviroment \n",
    "    Parameters:\n",
    "    - env: enviroment \n",
    "    - policy: Policy Neural Network \n",
    "    - batch_size: batch size\"\"\"\n",
    "\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    step_count = 0 \n",
    "    episode_step = []\n",
    "    obs = env.reset()[0] # ouput three value, x value, y value and Angular Velocity \n",
    "\n",
    "    while True:\n",
    "        abs_vector = torch.FloatTensor(np.array(obs))\n",
    "        agent_policy_action = Policy(abs_vector).detach().numpy() # Feed Neural Network\n",
    "        \n",
    "        # Passing Action into environment \n",
    "        next_obs, reward, is_done, _ , _ = env.step(agent_policy_action)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(observation=obs, action=agent_policy_action))\n",
    "    \n",
    "        if is_done or step_count >= max_episode_steps: # \n",
    "            batch.append(Episode(reward = episode_reward, steps = episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs =  env.reset()[0]\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "        \n",
    "        obs = next_obs\n",
    "        step_count += 1 \n",
    "\n",
    "\n",
    "def filter_batches (batches:collections.namedtuple , percentile:int)-> list:\n",
    "    \"\"\" Filter the elite or best Episode to retrain the NN\n",
    "    - batch : namedtuple, conataining the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches\"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batches))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for sample in batches:\n",
    "        if sample.reward >= reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, sample.steps))\n",
    "        train_act.extend(map(lambda step:step.action, sample.steps))\n",
    "\n",
    "    train_obs_vector = torch.FloatTensor(train_obs)\n",
    "    train_act_vector = torch.FloatTensor(train_act)\n",
    "\n",
    "    return train_obs_vector, train_act_vector, reward_bound, reward_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 8.681227970915115e-15, Reward Mean : -9.400084480318185, Reward bound : -0.44820316193696963\n",
      "iter_no ; 1, loss : 5.24832678486638e-16, Reward Mean : -3.7799627481683435, Reward bound : -1.087118357544834\n",
      "iter_no ; 2, loss : 1.0698513167766485e-15, Reward Mean : -3.9609341324854763, Reward bound : -1.5008150293345128\n",
      "iter_no ; 3, loss : 2.3213753188715833e-15, Reward Mean : -1.7397613233320461, Reward bound : -0.09917973205346153\n",
      "iter_no ; 4, loss : 6.459479608508399e-16, Reward Mean : -2.968583558231043, Reward bound : -1.1275535664720029\n",
      "iter_no ; 5, loss : 6.661338147750939e-16, Reward Mean : -5.246190551247389, Reward bound : -3.995646054127965\n",
      "iter_no ; 6, loss : 8.679925657758712e-16, Reward Mean : -2.527811020138029, Reward bound : -0.5120119591508759\n",
      "iter_no ; 7, loss : 8.478066589120579e-16, Reward Mean : -2.395224914280303, Reward bound : -1.0744417778791282\n",
      "iter_no ; 8, loss : 1.0900371707009025e-15, Reward Mean : -2.752302679789458, Reward bound : -0.7885262431644684\n",
      "iter_no ; 9, loss : 7.266914294874153e-16, Reward Mean : -2.7470436542117684, Reward bound : -0.5821110592623283\n",
      "iter_no ; 10, loss : 1.6350557560513537e-15, Reward Mean : -3.3777746216801514, Reward bound : -0.29090361817386606\n",
      "iter_no ; 11, loss : 1.5341263805509651e-15, Reward Mean : -2.461305979832731, Reward bound : -0.5186359963671532\n",
      "iter_no ; 12, loss : 8.074349510635498e-16, Reward Mean : -3.917304871987368, Reward bound : -1.1693703694371511\n",
      "iter_no ; 13, loss : 3.0682527611019302e-15, Reward Mean : -3.912359049876404, Reward bound : -1.5855946893173667\n",
      "iter_no ; 14, loss : 1.170780692277037e-15, Reward Mean : -2.5263836607882615, Reward bound : -0.43280944690947365\n",
      "iter_no ; 15, loss : 1.3120817756259338e-15, Reward Mean : -2.710107822570623, Reward bound : -1.0944942723616848\n",
      "iter_no ; 16, loss : 7.266914294874153e-16, Reward Mean : -3.729103985321909, Reward bound : -1.1770888266451185\n",
      "iter_no ; 17, loss : 9.689219412762598e-16, Reward Mean : -3.0894652523633614, Reward bound : -0.9977287485360524\n",
      "iter_no ; 18, loss : 1.6350557560513537e-15, Reward Mean : -2.3012694025148246, Reward bound : -1.2757551481304858\n",
      "iter_no ; 19, loss : 2.139702633553297e-15, Reward Mean : -3.254825549314594, Reward bound : -1.7557261823994277\n",
      "iter_no ; 20, loss : 1.7763568394002505e-15, Reward Mean : -3.898975788542094, Reward bound : -2.188797237003582\n",
      "iter_no ; 21, loss : 8.276208049878039e-16, Reward Mean : -2.561432097500342, Reward bound : -0.29585507800487065\n",
      "iter_no ; 22, loss : 1.1505947324736646e-15, Reward Mean : -3.1190335172406596, Reward bound : -0.789237282457117\n",
      "iter_no ; 23, loss : 4.037174755317749e-16, Reward Mean : -3.231428219685241, Reward bound : -0.55824840408124\n",
      "iter_no ; 24, loss : 7.468772834116693e-16, Reward Mean : -3.6313848804591675, Reward bound : -0.47304009982947487\n",
      "iter_no ; 25, loss : 9.083642736243793e-16, Reward Mean : -3.4318041758075557, Reward bound : -1.2299052053668733\n",
      "iter_no ; 26, loss : 1.1505947324736646e-15, Reward Mean : -3.405056379803522, Reward bound : -1.9177398280774014\n",
      "iter_no ; 27, loss : 2.200260195326059e-15, Reward Mean : -3.4279839701156902, Reward bound : -1.3188349905805763\n",
      "iter_no ; 28, loss : 7.266914294874153e-16, Reward Mean : -4.402454651731976, Reward bound : -2.204313471998579\n",
      "iter_no ; 29, loss : 1.2918959217016798e-15, Reward Mean : -3.241516873943242, Reward bound : -0.9816619803740121\n",
      "iter_no ; 30, loss : 1.2918959217016798e-15, Reward Mean : -3.8172260055928477, Reward bound : -2.1698064598871856\n",
      "iter_no ; 31, loss : 1.6956133178241159e-15, Reward Mean : -2.4190910081736683, Reward bound : -0.46134026827771135\n",
      "iter_no ; 32, loss : 7.670631902754826e-16, Reward Mean : -2.839380555021239, Reward bound : -0.6995751996652555\n",
      "iter_no ; 33, loss : 1.6754274638998618e-15, Reward Mean : -2.8046965149576515, Reward bound : -0.8433116208289013\n",
      "iter_no ; 34, loss : 1.3928252972020684e-15, Reward Mean : -2.6955359437534656, Reward bound : -0.9030087563968712\n",
      "iter_no ; 35, loss : 8.074349510635498e-16, Reward Mean : -5.009047695480011, Reward bound : -3.3878900795690408\n",
      "iter_no ; 36, loss : 5.853903461385185e-16, Reward Mean : -2.3270098221695217, Reward bound : -0.4529356097485262\n",
      "iter_no ; 37, loss : 5.24832678486638e-16, Reward Mean : -2.161659373611017, Reward bound : -0.22321683012438143\n",
      "iter_no ; 38, loss : 8.074349510635498e-16, Reward Mean : -2.537952438808671, Reward bound : -0.8184847062396987\n",
      "iter_no ; 39, loss : 4.440892098500626e-16, Reward Mean : -2.718552637648632, Reward bound : -1.062079746104811\n",
      "iter_no ; 40, loss : 2.0185873776588746e-16, Reward Mean : -3.5804090512532527, Reward bound : -1.138369157254724\n",
      "iter_no ; 41, loss : 2.03877315217379e-15, Reward Mean : -3.7001976659280365, Reward bound : -1.539706821965317\n",
      "iter_no ; 42, loss : 2.058959006098044e-15, Reward Mean : -2.6963037566204706, Reward bound : -1.3194336489678227\n",
      "iter_no ; 43, loss : 5.04646824562384e-16, Reward Mean : -3.1343360574331744, Reward bound : -0.6248849493666612\n",
      "iter_no ; 44, loss : 1.2918959217016798e-15, Reward Mean : -2.271407031530939, Reward bound : -0.4507904766443785\n",
      "iter_no ; 45, loss : 1.4533828589748305e-15, Reward Mean : -3.1284980698023146, Reward bound : -1.4856641383259976\n",
      "iter_no ; 46, loss : 1.3524534834744419e-15, Reward Mean : -3.129053402038595, Reward bound : -0.5800710326782887\n",
      "iter_no ; 47, loss : 3.835315951377413e-16, Reward Mean : -3.5193557321153754, Reward bound : -1.3840578951895295\n",
      "iter_no ; 48, loss : 2.0185873776588746e-17, Reward Mean : -4.431582330960385, Reward bound : -1.3045131021179743\n",
      "iter_no ; 49, loss : 8.276208049878039e-16, Reward Mean : -3.471499493819159, Reward bound : -1.0805647698740883\n",
      "iter_no ; 50, loss : 1.2111524001255452e-15, Reward Mean : -3.1166068993956912, Reward bound : -0.8996768299565427\n",
      "iter_no ; 51, loss : 2.5030482159481065e-15, Reward Mean : -3.2258245811309063, Reward bound : -1.433597905304981\n",
      "iter_no ; 52, loss : 2.4223048531906495e-16, Reward Mean : -2.4213022826268764, Reward bound : -0.7447651565496363\n",
      "iter_no ; 53, loss : 7.670631902754826e-16, Reward Mean : -3.596441166816346, Reward bound : -0.9983710690254218\n",
      "iter_no ; 54, loss : 1.6956133178241159e-15, Reward Mean : -4.2424167662876355, Reward bound : -1.864916476767289\n",
      "iter_no ; 55, loss : 2.5030482159481065e-15, Reward Mean : -2.7188447155730078, Reward bound : -0.7463418609861509\n",
      "iter_no ; 56, loss : 3.2297398042541994e-16, Reward Mean : -5.56136348634365, Reward bound : -4.2168420314775785\n",
      "iter_no ; 57, loss : 1.8167285737185382e-16, Reward Mean : -2.3025538394046468, Reward bound : -0.77251787949894\n",
      "iter_no ; 58, loss : 1.372639337398696e-15, Reward Mean : -3.845111601956323, Reward bound : -1.3799363195628638\n",
      "iter_no ; 59, loss : 2.099330713946552e-15, Reward Mean : -2.6246307572703333, Reward bound : -0.5325171046358623\n",
      "iter_no ; 60, loss : 1.1304088785494106e-15, Reward Mean : -3.67736700411158, Reward bound : -0.8384902397412769\n",
      "iter_no ; 61, loss : 1.6148699021270997e-15, Reward Mean : -4.565239659609247, Reward bound : -3.115035218620929\n",
      "iter_no ; 62, loss : 1.8167285472487586e-15, Reward Mean : -2.3096573041099573, Reward bound : -0.8809818134279346\n",
      "iter_no ; 63, loss : 9.083642736243793e-16, Reward Mean : -3.2788561754680856, Reward bound : -0.7062997820743604\n",
      "iter_no ; 64, loss : 5.04646824562384e-16, Reward Mean : -3.078543917359241, Reward bound : -0.8508915020145552\n",
      "iter_no ; 65, loss : 1.2111524001255452e-15, Reward Mean : -3.805757874084978, Reward bound : -0.9364181051622342\n",
      "iter_no ; 66, loss : 5.04646824562384e-16, Reward Mean : -3.2036533920940022, Reward bound : -0.9551513855225455\n",
      "iter_no ; 67, loss : 7.266914294874153e-16, Reward Mean : -4.256471815517327, Reward bound : -1.5907609830616063\n",
      "iter_no ; 68, loss : 9.689219412762598e-16, Reward Mean : -4.312953047844566, Reward bound : -2.477294737080536\n",
      "iter_no ; 69, loss : 8.478066589120579e-16, Reward Mean : -3.5899521337761406, Reward bound : -0.9866370630384259\n",
      "iter_no ; 70, loss : 1.2918959217016798e-15, Reward Mean : -3.9239399906418173, Reward bound : -2.0806214584136757\n",
      "iter_no ; 71, loss : 1.2515241079740533e-15, Reward Mean : -2.5864122563424266, Reward bound : -0.5241039998706467\n",
      "iter_no ; 72, loss : 4.844609706381299e-16, Reward Mean : -4.092039988145038, Reward bound : -1.2866075119944986\n",
      "iter_no ; 73, loss : 6.661338147750939e-16, Reward Mean : -3.3732826082987337, Reward bound : -1.6967396739002507\n",
      "iter_no ; 74, loss : 3.1489961767989464e-15, Reward Mean : -3.1121248530681944, Reward bound : -0.8541217852090704\n",
      "iter_no ; 75, loss : 1.1304088785494106e-15, Reward Mean : -2.989874571799441, Reward bound : -0.5726791345243021\n",
      "iter_no ; 76, loss : 1.4533828589748305e-15, Reward Mean : -3.12249154086214, Reward bound : -0.7047659767876389\n",
      "iter_no ; 77, loss : 6.661338147750939e-16, Reward Mean : -2.7786867156912987, Reward bound : -0.793663603361583\n",
      "iter_no ; 78, loss : 8.074349510635498e-16, Reward Mean : -3.4203810146588074, Reward bound : -0.56749733518352\n",
      "iter_no ; 79, loss : 7.266914294874153e-16, Reward Mean : -4.697021348079536, Reward bound : -3.0371373264344794\n",
      "iter_no ; 80, loss : 2.4424906541753444e-15, Reward Mean : -3.430911558601202, Reward bound : -0.9547438663630183\n",
      "iter_no ; 81, loss : 6.459479608508399e-16, Reward Mean : -5.022270700210611, Reward bound : -2.5030655076520665\n",
      "iter_no ; 82, loss : 3.2297398042541994e-16, Reward Mean : -4.121416000658888, Reward bound : -1.4686663528919075\n",
      "iter_no ; 83, loss : 6.661338147750939e-16, Reward Mean : -2.1722649030600336, Reward bound : -0.2238157890794119\n",
      "iter_no ; 84, loss : 6.459479608508399e-16, Reward Mean : -4.078241511121512, Reward bound : -2.6698755166167194\n",
      "iter_no ; 85, loss : 5.04646824562384e-16, Reward Mean : -4.242079037612903, Reward bound : -2.075736619205943\n",
      "iter_no ; 86, loss : 8.881784197001252e-16, Reward Mean : -3.5347216399410533, Reward bound : -1.3718512204105204\n",
      "iter_no ; 87, loss : 1.71579917174837e-15, Reward Mean : -3.3272447751722956, Reward bound : -1.7359216914772764\n",
      "iter_no ; 88, loss : 1.9378438825525196e-15, Reward Mean : -3.945025501446417, Reward bound : -1.9674698851707415\n",
      "iter_no ; 89, loss : 8.074349510635498e-16, Reward Mean : -2.2038467205090164, Reward bound : -0.40251893003275996\n",
      "iter_no ; 90, loss : 1.6350557560513537e-15, Reward Mean : -2.684651857396874, Reward bound : -0.3079636246969202\n",
      "iter_no ; 91, loss : 1.6148699021270997e-15, Reward Mean : -3.719312416201975, Reward bound : -0.299123153821619\n",
      "iter_no ; 92, loss : 2.2810036110230752e-15, Reward Mean : -3.7392666125111615, Reward bound : -0.6564059201540173\n",
      "iter_no ; 93, loss : 1.513940526626711e-15, Reward Mean : -3.202423930146039, Reward bound : -1.5930229524559938\n",
      "iter_no ; 94, loss : 4.2390332945602897e-16, Reward Mean : -3.3051401990985703, Reward bound : -1.0692647129925352\n",
      "iter_no ; 95, loss : 2.341561384554074e-15, Reward Mean : -3.830875427211014, Reward bound : -1.0220710384817686\n",
      "iter_no ; 96, loss : 8.074349510635498e-16, Reward Mean : -2.5403071118042595, Reward bound : -0.7585568884678318\n",
      "iter_no ; 97, loss : 1.9378438825525196e-15, Reward Mean : -4.222723847004124, Reward bound : -2.029278804430488\n",
      "iter_no ; 98, loss : 1.6956133178241159e-15, Reward Mean : -3.715480793486097, Reward bound : -1.1211313475943596\n",
      "iter_no ; 99, loss : 6.661338147750939e-16, Reward Mean : -1.543994274205557, Reward bound : -0.3311099903916692\n",
      "iter_no ; 100, loss : 2.099330713946552e-15, Reward Mean : -3.2423822066676005, Reward bound : -0.3409376876894478\n",
      "iter_no ; 101, loss : 2.099330713946552e-15, Reward Mean : -3.7072410600684, Reward bound : -1.3587477780470794\n",
      "iter_no ; 102, loss : 1.049665356973276e-15, Reward Mean : -2.9760005241542196, Reward bound : -0.4013842906147679\n",
      "iter_no ; 103, loss : 8.881784197001252e-16, Reward Mean : -3.511683949746389, Reward bound : -2.307007807150571\n",
      "iter_no ; 104, loss : 9.285501275486333e-16, Reward Mean : -3.2386728622037624, Reward bound : -0.6998789309585027\n",
      "iter_no ; 105, loss : 8.074349510635498e-17, Reward Mean : -3.3641084135535726, Reward bound : -0.9824669972644442\n",
      "iter_no ; 106, loss : 2.0185873776588746e-17, Reward Mean : -3.14491423513094, Reward bound : -0.9539662315804065\n",
      "iter_no ; 107, loss : 1.4533828589748305e-15, Reward Mean : -2.04221538207991, Reward bound : -0.6074870564177803\n",
      "iter_no ; 108, loss : 6.459479608508399e-16, Reward Mean : -3.0256901262191445, Reward bound : -1.1710643932244993\n",
      "iter_no ; 109, loss : 2.03877315217379e-15, Reward Mean : -3.7023251097663925, Reward bound : -0.9156741270082863\n",
      "iter_no ; 110, loss : 4.037174755317749e-16, Reward Mean : -4.0069216766630635, Reward bound : -1.1483040636560962\n",
      "iter_no ; 111, loss : 6.459479608508399e-16, Reward Mean : -4.115359729599808, Reward bound : -2.2805991256217792\n",
      "iter_no ; 112, loss : 1.2313382540497992e-15, Reward Mean : -3.9674580328254816, Reward bound : -1.4132502604469968\n",
      "iter_no ; 113, loss : 1.2918959217016798e-15, Reward Mean : -4.716452752986557, Reward bound : -1.4487904690762314\n",
      "iter_no ; 114, loss : 1.4735687128990846e-15, Reward Mean : -4.209586370991371, Reward bound : -1.3081177542335383\n",
      "iter_no ; 115, loss : 9.689219412762598e-16, Reward Mean : -2.9324917163490394, Reward bound : -0.8691831735411307\n",
      "iter_no ; 116, loss : 1.9378438825525196e-15, Reward Mean : -3.5757335907998637, Reward bound : -1.5835316647335171\n",
      "iter_no ; 117, loss : 1.2111524001255452e-15, Reward Mean : -3.6882464060510167, Reward bound : -1.0388835872177693\n",
      "iter_no ; 118, loss : 1.2111524001255452e-15, Reward Mean : -3.348707581143768, Reward bound : -1.0985050025791372\n",
      "iter_no ; 119, loss : 8.276208049878039e-16, Reward Mean : -2.873064662164337, Reward bound : -0.8215595138412397\n",
      "iter_no ; 120, loss : 2.099330713946552e-15, Reward Mean : -4.121716009267543, Reward bound : -2.035838192895861\n",
      "iter_no ; 121, loss : 4.037174755317749e-16, Reward Mean : -3.3878817208681298, Reward bound : -1.8307903754387942\n",
      "iter_no ; 122, loss : 1.6148699021270997e-15, Reward Mean : -3.3105397098482543, Reward bound : -1.483799169501196\n",
      "iter_no ; 123, loss : 1.049665356973276e-15, Reward Mean : -3.8235144889228265, Reward bound : -2.141805305926273\n",
      "iter_no ; 124, loss : 9.689219412762598e-16, Reward Mean : -3.837007320247669, Reward bound : -1.1887225761577649\n",
      "iter_no ; 125, loss : 1.1505947324736646e-15, Reward Mean : -3.1369168183352896, Reward bound : -0.8766903450372885\n",
      "iter_no ; 126, loss : 6.661338147750939e-16, Reward Mean : -3.0014676474931488, Reward bound : -1.7820845158741299\n",
      "iter_no ; 127, loss : 1.049665356973276e-15, Reward Mean : -3.7682138196811117, Reward bound : -1.0343726613141857\n",
      "iter_no ; 128, loss : 9.083642736243793e-16, Reward Mean : -2.7731712421399664, Reward bound : -0.6027893791992714\n",
      "iter_no ; 129, loss : 3.43159834349674e-16, Reward Mean : -4.894144048992859, Reward bound : -2.182397106858893\n",
      "iter_no ; 130, loss : 2.5837918434033595e-15, Reward Mean : -3.3337784413012543, Reward bound : -0.38471508255434966\n",
      "iter_no ; 131, loss : 8.074349510635498e-16, Reward Mean : -2.613229890609327, Reward bound : -1.038426874865615\n",
      "iter_no ; 132, loss : 4.037174755317749e-16, Reward Mean : -3.1285914332036815, Reward bound : -0.36940370246652277\n",
      "iter_no ; 133, loss : 5.04646824562384e-16, Reward Mean : -2.950414459981749, Reward bound : -0.7231649362288606\n",
      "iter_no ; 134, loss : 1.0698513167766485e-15, Reward Mean : -2.9494282676932033, Reward bound : -0.8542355596539892\n",
      "iter_no ; 135, loss : 8.074349510635498e-16, Reward Mean : -3.318137215054486, Reward bound : -0.5846709446085371\n",
      "iter_no ; 136, loss : 1.049665356973276e-15, Reward Mean : -3.1533208409822664, Reward bound : -1.9230500842183362\n",
      "iter_no ; 137, loss : 4.844609706381299e-16, Reward Mean : -2.4080605450787074, Reward bound : -1.1363829843931583\n",
      "iter_no ; 138, loss : 9.083642736243793e-16, Reward Mean : -2.8513427575884718, Reward bound : -1.2443072299205893\n",
      "iter_no ; 139, loss : 2.4223048002510903e-15, Reward Mean : -3.622138381637323, Reward bound : -2.048715502804235\n",
      "iter_no ; 140, loss : 1.6956133178241159e-15, Reward Mean : -3.6628578121644044, Reward bound : -1.0013122810101183\n",
      "iter_no ; 141, loss : 1.2111524001255452e-15, Reward Mean : -2.8373976334381297, Reward bound : -0.07618881429309511\n",
      "iter_no ; 142, loss : 9.689219412762598e-16, Reward Mean : -2.0161173422408707, Reward bound : -0.8176078758996081\n",
      "iter_no ; 143, loss : 2.6645352591003757e-15, Reward Mean : -3.2035634670487605, Reward bound : -0.6526735071610965\n",
      "iter_no ; 144, loss : 8.881784197001252e-16, Reward Mean : -2.6735534458128773, Reward bound : -1.2191163684804245\n",
      "iter_no ; 145, loss : 1.1304088785494106e-15, Reward Mean : -4.008382420384552, Reward bound : -1.6537739371767783\n",
      "iter_no ; 146, loss : 7.266914294874153e-16, Reward Mean : -2.986946140325232, Reward bound : -2.8274163481481143\n",
      "iter_no ; 147, loss : 2.341561384554074e-15, Reward Mean : -3.184219902799061, Reward bound : -2.0845923074078314\n",
      "iter_no ; 148, loss : 1.7460780585138694e-15, Reward Mean : -2.22988804448243, Reward bound : -0.70387142233237\n",
      "iter_no ; 149, loss : 8.276208049878039e-16, Reward Mean : -2.8823246444030173, Reward bound : -1.2557170643430426\n",
      "iter_no ; 150, loss : 7.266914294874153e-16, Reward Mean : -4.56396772255402, Reward bound : -1.578454816726909\n",
      "iter_no ; 151, loss : 1.6148699021270997e-15, Reward Mean : -3.967955605022456, Reward bound : -1.5256136318417122\n",
      "iter_no ; 152, loss : 6.459479608508399e-16, Reward Mean : -3.3983111433425126, Reward bound : -0.45737445946338473\n",
      "iter_no ; 153, loss : 3.43159834349674e-16, Reward Mean : -3.395711251788256, Reward bound : -1.2250279549859289\n",
      "iter_no ; 154, loss : 2.180074341401805e-15, Reward Mean : -2.621200860400501, Reward bound : -1.0463105737487868\n",
      "iter_no ; 155, loss : 1.2313382540497992e-15, Reward Mean : -2.6570242921374705, Reward bound : -0.3153620276190216\n",
      "iter_no ; 156, loss : 2.341561384554074e-15, Reward Mean : -3.426011565211205, Reward bound : -2.038233554267343\n",
      "iter_no ; 157, loss : 4.844609706381299e-16, Reward Mean : -3.8363467135074796, Reward bound : -1.4924037542491169\n",
      "iter_no ; 158, loss : 1.7965426933245045e-15, Reward Mean : -3.1625322130431015, Reward bound : -1.4866580992235154\n",
      "iter_no ; 159, loss : 1.009293649124768e-15, Reward Mean : -2.7296193178264354, Reward bound : -1.7289682594169204\n",
      "iter_no ; 160, loss : 2.5232340698723606e-15, Reward Mean : -2.980558604717073, Reward bound : -0.8754650785344262\n",
      "iter_no ; 161, loss : 1.3928252972020684e-15, Reward Mean : -3.3616445493872114, Reward bound : -0.9656702120639553\n",
      "iter_no ; 162, loss : 5.24832678486638e-16, Reward Mean : -3.623965713051035, Reward bound : -1.2123125951286875\n",
      "iter_no ; 163, loss : 8.074349510635498e-17, Reward Mean : -3.831007712577315, Reward bound : -2.4237962373754467\n",
      "iter_no ; 164, loss : 1.049665356973276e-15, Reward Mean : -3.4888546572094956, Reward bound : -0.9746922709982315\n",
      "iter_no ; 165, loss : 1.5543122344752192e-15, Reward Mean : -3.6919453551636003, Reward bound : -1.4955744782858749\n",
      "iter_no ; 166, loss : 1.1505947324736646e-15, Reward Mean : -3.4619164628332406, Reward bound : -1.5599854127585981\n",
      "iter_no ; 167, loss : 2.4223048002510903e-15, Reward Mean : -2.40817240938107, Reward bound : -0.36168483817541497\n",
      "iter_no ; 168, loss : 1.8369144011730126e-15, Reward Mean : -2.856080851326184, Reward bound : -0.7169751398353228\n",
      "iter_no ; 169, loss : 1.8167285737185382e-16, Reward Mean : -2.07952874874269, Reward bound : -0.7696908327242801\n",
      "iter_no ; 170, loss : 2.084191482322039e-15, Reward Mean : -3.619424517204708, Reward bound : -1.840662935377762\n",
      "iter_no ; 171, loss : 8.276208049878039e-16, Reward Mean : -3.9490042103148584, Reward bound : -2.109462412819827\n",
      "iter_no ; 172, loss : 3.4719702631034847e-15, Reward Mean : -2.9688430602713813, Reward bound : -0.8373350279075693\n",
      "iter_no ; 173, loss : 9.689219412762598e-16, Reward Mean : -4.353003972330826, Reward bound : -2.6162731155923424\n",
      "iter_no ; 174, loss : 5.652044392747053e-16, Reward Mean : -3.5013906646862836, Reward bound : -1.1677605310555528\n",
      "iter_no ; 175, loss : 2.745278674797392e-15, Reward Mean : -3.890315268326724, Reward bound : -1.9050252170987205\n",
      "iter_no ; 176, loss : 3.2297398042541994e-16, Reward Mean : -4.646003463863442, Reward bound : -2.848991259534653\n",
      "iter_no ; 177, loss : 2.099330713946552e-15, Reward Mean : -3.015082379793835, Reward bound : -1.3794740224187394\n",
      "iter_no ; 178, loss : 6.055762000627726e-16, Reward Mean : -3.3185014395722368, Reward bound : -0.8733441575186889\n",
      "iter_no ; 179, loss : 9.891077952005139e-16, Reward Mean : -4.530748843295129, Reward bound : -1.9641972256341627\n",
      "iter_no ; 180, loss : 1.8571002550972666e-15, Reward Mean : -2.9312721509770956, Reward bound : -0.8078881612198938\n",
      "iter_no ; 181, loss : 1.2111524001255452e-15, Reward Mean : -3.3435582912443413, Reward bound : -1.0995447275711874\n",
      "iter_no ; 182, loss : 9.689219412762598e-16, Reward Mean : -1.879556470715564, Reward bound : -0.12683257730794784\n",
      "iter_no ; 183, loss : 8.074349510635498e-16, Reward Mean : -2.9990971249778204, Reward bound : -0.6641342811590432\n",
      "iter_no ; 184, loss : 1.049665356973276e-15, Reward Mean : -4.711526608859339, Reward bound : -2.2286760331789943\n",
      "iter_no ; 185, loss : 1.1304088785494106e-15, Reward Mean : -3.1820303043351204, Reward bound : -1.1691941347207047\n",
      "iter_no ; 186, loss : 9.689219412762598e-16, Reward Mean : -4.8440237756690525, Reward bound : -2.985859282763966\n",
      "iter_no ; 187, loss : 1.6148699021270997e-15, Reward Mean : -4.595198115039567, Reward bound : -2.0321237464415827\n",
      "iter_no ; 188, loss : 2.200260195326059e-15, Reward Mean : -1.5450503025174673, Reward bound : -0.3322973772502789\n",
      "iter_no ; 189, loss : 0.0, Reward Mean : -4.114252719676422, Reward bound : -1.4888374584553392\n",
      "iter_no ; 190, loss : 1.2918959217016798e-15, Reward Mean : -5.082916837839716, Reward bound : -3.92635492909472\n",
      "iter_no ; 191, loss : 5.298791949072607e-16, Reward Mean : -3.440199386599494, Reward bound : -1.7002549541653385\n",
      "iter_no ; 192, loss : 2.2810036110230752e-15, Reward Mean : -2.503926562040365, Reward bound : -1.4907486911325667\n",
      "iter_no ; 193, loss : 1.6148699021270997e-16, Reward Mean : -3.5170124734776866, Reward bound : -1.9513425426272621\n",
      "iter_no ; 194, loss : 9.083642736243793e-16, Reward Mean : -2.3292190340848764, Reward bound : -0.3713835897200539\n",
      "iter_no ; 195, loss : 8.276208049878039e-16, Reward Mean : -3.535060726410375, Reward bound : -0.5876293364698837\n",
      "iter_no ; 196, loss : 4.037174755317749e-16, Reward Mean : -4.1867558357603825, Reward bound : -2.2963479229857375\n",
      "iter_no ; 197, loss : 2.099330713946552e-15, Reward Mean : -3.3753440213475985, Reward bound : -1.5433412611734085\n",
      "iter_no ; 198, loss : 4.844609706381299e-16, Reward Mean : -1.1724473363362942, Reward bound : -0.1880732196924379\n",
      "solved\n"
     ]
    }
   ],
   "source": [
    "# Training Agent \n",
    "HIDDEN_LAYER = 150\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = collections.namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = collections.namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiate Environment\n",
    "env = gym.make('Pendulum-v1', render_mode = 'human')\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions_ = env.action_space.shape[0]\n",
    "\n",
    "# Intiate Neural Network , Loss Function and Optimizer\n",
    "Net = PolicyNeuralNetwork(obs_size=obs_size, hidden_size=HIDDEN_LAYER, n_actions=n_actions_)\n",
    "objective = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=Net.parameters(), lr=0.001)\n",
    "\n",
    "# Training NN and Agent \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Policy=Net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying Optimization \n",
    "    obs_vector, action_vector, reward_boundary, reward_mean = filter_batches(batches = batch, percentile=PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_score_vector = Net(obs_vector) # trainining only on the best\n",
    "\n",
    "    loss_vector = objective(action_score_vector, action_vector)\n",
    "    loss_vector.backward() # back propagation\n",
    "    optimizer.step() # applying and update \n",
    "    time.sleep(1/10)\n",
    "    env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_vector.item()}, Reward Mean : {reward_mean}, Reward bound : {reward_boundary}\")\n",
    "\n",
    "    if reward_mean > -1.5:\n",
    "        print(\"solved\")\n",
    "        break\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ the \\ Mountain \\ Car \\ Problem \\ with \\ Cross \\ Entropy \\ - \\ Functions $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_reward (position:int) -> int:\n",
    "    \"\"\" Function that transform the reward according to the the Min position from observation \n",
    "    Args:\n",
    "        position : min position given by the environment\n",
    "\n",
    "    Return \n",
    "        reward  float, \n",
    "    \"\"\"\n",
    "\n",
    "    reward_values = {-0.1000000:6,0.1100000:5,-0.1300000:4,-0.1500000:3,0.1700000:2,-0.2000000:1}\n",
    "    for location, reward in reward_values.items():\n",
    "        if position >= location:\n",
    "            return reward\n",
    "        \n",
    "    return -1\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal NN Policy mapper decision for the agent to choose an action \"\"\"\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.LeakyReLU (),\n",
    "            nn.Linear(in_features=hidden_size, out_features=550),\n",
    "            nn.LeakyReLU (),\n",
    "            nn.Linear(in_features=550, out_features=n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, X:torch.FloatTensor)-> torch.FloatTensor:\n",
    "        return self.policy(X)\n",
    "    \n",
    "\n",
    "def batches_iterator (env:gym.make, AgentPolicy: nn.Module, batch_size:int):\n",
    "    \"\"\"\n",
    "    Iterate through batches of episodes.\n",
    "\n",
    "    Args:\n",
    "        env: The gym environment to use.\n",
    "        policy_network: The neural network that defines the agent's policy.\n",
    "        batch_size: The number of episodes per batch.\n",
    "\n",
    "    Yields:\n",
    "        A batch of episodes, represented as a list of Episode objects.\n",
    "    \"\"\"\n",
    "\n",
    "    batch , episode_step = [], []\n",
    "    episode_reward, step_count = 0.0 , 0\n",
    "    obs = env.reset()[0] # the new version return the output as tuple \n",
    "    activation_function = nn.Softmax(dim=1) # activation function \n",
    "\n",
    "    while True:\n",
    "        obs_vector = torch.FloatTensor(np.array([obs]))\n",
    "        act_prob_vector = activation_function(AgentPolicy(obs_vector)) # get the output probabilities from Neural Network\n",
    "        act_prob = act_prob_vector.detach().numpy()[0] \n",
    "        action = np.random.choice(len(act_prob), p= act_prob)\n",
    "\n",
    "        # Environment\n",
    "        next_obs, rewards , is_done, _, _ = env.step(action)\n",
    "        episode_reward += transform_reward(rewards)\n",
    "\n",
    "        # Track the best steps \n",
    "        episode_step.append(EpisodeStep(observation = obs, action = action))\n",
    "        \n",
    "        # Reset\n",
    "        if is_done or step_count > EPISODES:\n",
    "            batch.append(Episode(reward=episode_reward, steps = episode_step))\n",
    "            episode_reward = 0.0 \n",
    "            next_obs = env.reset()[0]\n",
    "            episode_step = []\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs\n",
    "        step_count += 1 \n",
    "\n",
    "def filter_batches (batch:namedtuple, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # Get those reward that are above the percentile \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for example in batch:\n",
    "        if example.reward > reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step:step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
