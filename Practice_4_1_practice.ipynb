{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Functions \\ and \\ Classes $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal NN Policy mapper decision for the agent to choose an action \"\"\"\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(PolicyNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, X:torch.FloatTensor)-> torch.FloatTensor:\n",
    "        return torch.tanh(self.policy(X))*2 # Tanh activation maps the output to the range (-1, 1) , # Scale the output to the range (-2, 2)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ the \\ Pendulum \\ Problem \\ with \\ Cross \\ Entropy $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env:gym.make, Policy:nn.Module, batch_size:int, max_episode_steps:int=20):\n",
    "    \"\"\" Iterate thorugh batches and run the enviroment \n",
    "    Parameters:\n",
    "    - env: enviroment \n",
    "    - policy: Policy Neural Network \n",
    "    - batch_size: batch size\"\"\"\n",
    "\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    step_count = 0 \n",
    "    episode_step = []\n",
    "    obs = env.reset()[0] # ouput three value, x value, y value and Angular Velocity \n",
    "\n",
    "    while True:\n",
    "        abs_vector = torch.FloatTensor(np.array(obs))\n",
    "        agent_policy_action = Policy(abs_vector).detach().numpy() # Feed Neural Network\n",
    "        \n",
    "        # Passing Action into environment \n",
    "        next_obs, reward, is_done, _ , _ = env.step(agent_policy_action)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(observation=obs, action=agent_policy_action))\n",
    "    \n",
    "        if is_done or step_count >= max_episode_steps: # \n",
    "            batch.append(Episode(reward = episode_reward, steps = episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs =  env.reset()[0]\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "        \n",
    "        obs = next_obs\n",
    "        step_count += 1 \n",
    "\n",
    "\n",
    "def filter_batches (batches:collections.namedtuple , percentile:int)-> list:\n",
    "    \"\"\" Filter the elite or best Episode to retrain the NN\n",
    "    - batch : namedtuple, conataining the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches\"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batches))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for sample in batches:\n",
    "        if sample.reward >= reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, sample.steps))\n",
    "        train_act.extend(map(lambda step:step.action, sample.steps))\n",
    "\n",
    "    train_obs_vector = torch.FloatTensor(train_obs)\n",
    "    train_act_vector = torch.FloatTensor(train_act)\n",
    "\n",
    "    return train_obs_vector, train_act_vector, reward_bound, reward_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 8.681227970915115e-15, Reward Mean : -9.400084480318185, Reward bound : -0.44820316193696963\n",
      "iter_no ; 1, loss : 5.24832678486638e-16, Reward Mean : -3.7799627481683435, Reward bound : -1.087118357544834\n",
      "iter_no ; 2, loss : 1.0698513167766485e-15, Reward Mean : -3.9609341324854763, Reward bound : -1.5008150293345128\n",
      "iter_no ; 3, loss : 2.3213753188715833e-15, Reward Mean : -1.7397613233320461, Reward bound : -0.09917973205346153\n",
      "iter_no ; 4, loss : 6.459479608508399e-16, Reward Mean : -2.968583558231043, Reward bound : -1.1275535664720029\n",
      "iter_no ; 5, loss : 6.661338147750939e-16, Reward Mean : -5.246190551247389, Reward bound : -3.995646054127965\n",
      "iter_no ; 6, loss : 8.679925657758712e-16, Reward Mean : -2.527811020138029, Reward bound : -0.5120119591508759\n",
      "iter_no ; 7, loss : 8.478066589120579e-16, Reward Mean : -2.395224914280303, Reward bound : -1.0744417778791282\n",
      "iter_no ; 8, loss : 1.0900371707009025e-15, Reward Mean : -2.752302679789458, Reward bound : -0.7885262431644684\n",
      "iter_no ; 9, loss : 7.266914294874153e-16, Reward Mean : -2.7470436542117684, Reward bound : -0.5821110592623283\n",
      "iter_no ; 10, loss : 1.6350557560513537e-15, Reward Mean : -3.3777746216801514, Reward bound : -0.29090361817386606\n",
      "iter_no ; 11, loss : 1.5341263805509651e-15, Reward Mean : -2.461305979832731, Reward bound : -0.5186359963671532\n",
      "iter_no ; 12, loss : 8.074349510635498e-16, Reward Mean : -3.917304871987368, Reward bound : -1.1693703694371511\n",
      "iter_no ; 13, loss : 3.0682527611019302e-15, Reward Mean : -3.912359049876404, Reward bound : -1.5855946893173667\n",
      "iter_no ; 14, loss : 1.170780692277037e-15, Reward Mean : -2.5263836607882615, Reward bound : -0.43280944690947365\n",
      "iter_no ; 15, loss : 1.3120817756259338e-15, Reward Mean : -2.710107822570623, Reward bound : -1.0944942723616848\n",
      "iter_no ; 16, loss : 7.266914294874153e-16, Reward Mean : -3.729103985321909, Reward bound : -1.1770888266451185\n",
      "iter_no ; 17, loss : 9.689219412762598e-16, Reward Mean : -3.0894652523633614, Reward bound : -0.9977287485360524\n",
      "iter_no ; 18, loss : 1.6350557560513537e-15, Reward Mean : -2.3012694025148246, Reward bound : -1.2757551481304858\n",
      "iter_no ; 19, loss : 2.139702633553297e-15, Reward Mean : -3.254825549314594, Reward bound : -1.7557261823994277\n",
      "iter_no ; 20, loss : 1.7763568394002505e-15, Reward Mean : -3.898975788542094, Reward bound : -2.188797237003582\n",
      "iter_no ; 21, loss : 8.276208049878039e-16, Reward Mean : -2.561432097500342, Reward bound : -0.29585507800487065\n",
      "iter_no ; 22, loss : 1.1505947324736646e-15, Reward Mean : -3.1190335172406596, Reward bound : -0.789237282457117\n",
      "iter_no ; 23, loss : 4.037174755317749e-16, Reward Mean : -3.231428219685241, Reward bound : -0.55824840408124\n",
      "iter_no ; 24, loss : 7.468772834116693e-16, Reward Mean : -3.6313848804591675, Reward bound : -0.47304009982947487\n",
      "iter_no ; 25, loss : 9.083642736243793e-16, Reward Mean : -3.4318041758075557, Reward bound : -1.2299052053668733\n",
      "iter_no ; 26, loss : 1.1505947324736646e-15, Reward Mean : -3.405056379803522, Reward bound : -1.9177398280774014\n",
      "iter_no ; 27, loss : 2.200260195326059e-15, Reward Mean : -3.4279839701156902, Reward bound : -1.3188349905805763\n",
      "iter_no ; 28, loss : 7.266914294874153e-16, Reward Mean : -4.402454651731976, Reward bound : -2.204313471998579\n",
      "iter_no ; 29, loss : 1.2918959217016798e-15, Reward Mean : -3.241516873943242, Reward bound : -0.9816619803740121\n",
      "iter_no ; 30, loss : 1.2918959217016798e-15, Reward Mean : -3.8172260055928477, Reward bound : -2.1698064598871856\n",
      "iter_no ; 31, loss : 1.6956133178241159e-15, Reward Mean : -2.4190910081736683, Reward bound : -0.46134026827771135\n",
      "iter_no ; 32, loss : 7.670631902754826e-16, Reward Mean : -2.839380555021239, Reward bound : -0.6995751996652555\n",
      "iter_no ; 33, loss : 1.6754274638998618e-15, Reward Mean : -2.8046965149576515, Reward bound : -0.8433116208289013\n",
      "iter_no ; 34, loss : 1.3928252972020684e-15, Reward Mean : -2.6955359437534656, Reward bound : -0.9030087563968712\n",
      "iter_no ; 35, loss : 8.074349510635498e-16, Reward Mean : -5.009047695480011, Reward bound : -3.3878900795690408\n",
      "iter_no ; 36, loss : 5.853903461385185e-16, Reward Mean : -2.3270098221695217, Reward bound : -0.4529356097485262\n",
      "iter_no ; 37, loss : 5.24832678486638e-16, Reward Mean : -2.161659373611017, Reward bound : -0.22321683012438143\n",
      "iter_no ; 38, loss : 8.074349510635498e-16, Reward Mean : -2.537952438808671, Reward bound : -0.8184847062396987\n",
      "iter_no ; 39, loss : 4.440892098500626e-16, Reward Mean : -2.718552637648632, Reward bound : -1.062079746104811\n",
      "iter_no ; 40, loss : 2.0185873776588746e-16, Reward Mean : -3.5804090512532527, Reward bound : -1.138369157254724\n",
      "iter_no ; 41, loss : 2.03877315217379e-15, Reward Mean : -3.7001976659280365, Reward bound : -1.539706821965317\n",
      "iter_no ; 42, loss : 2.058959006098044e-15, Reward Mean : -2.6963037566204706, Reward bound : -1.3194336489678227\n",
      "iter_no ; 43, loss : 5.04646824562384e-16, Reward Mean : -3.1343360574331744, Reward bound : -0.6248849493666612\n",
      "iter_no ; 44, loss : 1.2918959217016798e-15, Reward Mean : -2.271407031530939, Reward bound : -0.4507904766443785\n",
      "iter_no ; 45, loss : 1.4533828589748305e-15, Reward Mean : -3.1284980698023146, Reward bound : -1.4856641383259976\n",
      "iter_no ; 46, loss : 1.3524534834744419e-15, Reward Mean : -3.129053402038595, Reward bound : -0.5800710326782887\n",
      "iter_no ; 47, loss : 3.835315951377413e-16, Reward Mean : -3.5193557321153754, Reward bound : -1.3840578951895295\n",
      "iter_no ; 48, loss : 2.0185873776588746e-17, Reward Mean : -4.431582330960385, Reward bound : -1.3045131021179743\n",
      "iter_no ; 49, loss : 8.276208049878039e-16, Reward Mean : -3.471499493819159, Reward bound : -1.0805647698740883\n",
      "iter_no ; 50, loss : 1.2111524001255452e-15, Reward Mean : -3.1166068993956912, Reward bound : -0.8996768299565427\n",
      "iter_no ; 51, loss : 2.5030482159481065e-15, Reward Mean : -3.2258245811309063, Reward bound : -1.433597905304981\n",
      "iter_no ; 52, loss : 2.4223048531906495e-16, Reward Mean : -2.4213022826268764, Reward bound : -0.7447651565496363\n",
      "iter_no ; 53, loss : 7.670631902754826e-16, Reward Mean : -3.596441166816346, Reward bound : -0.9983710690254218\n",
      "iter_no ; 54, loss : 1.6956133178241159e-15, Reward Mean : -4.2424167662876355, Reward bound : -1.864916476767289\n",
      "iter_no ; 55, loss : 2.5030482159481065e-15, Reward Mean : -2.7188447155730078, Reward bound : -0.7463418609861509\n",
      "iter_no ; 56, loss : 3.2297398042541994e-16, Reward Mean : -5.56136348634365, Reward bound : -4.2168420314775785\n",
      "iter_no ; 57, loss : 1.8167285737185382e-16, Reward Mean : -2.3025538394046468, Reward bound : -0.77251787949894\n",
      "iter_no ; 58, loss : 1.372639337398696e-15, Reward Mean : -3.845111601956323, Reward bound : -1.3799363195628638\n",
      "iter_no ; 59, loss : 2.099330713946552e-15, Reward Mean : -2.6246307572703333, Reward bound : -0.5325171046358623\n",
      "iter_no ; 60, loss : 1.1304088785494106e-15, Reward Mean : -3.67736700411158, Reward bound : -0.8384902397412769\n",
      "iter_no ; 61, loss : 1.6148699021270997e-15, Reward Mean : -4.565239659609247, Reward bound : -3.115035218620929\n",
      "iter_no ; 62, loss : 1.8167285472487586e-15, Reward Mean : -2.3096573041099573, Reward bound : -0.8809818134279346\n",
      "iter_no ; 63, loss : 9.083642736243793e-16, Reward Mean : -3.2788561754680856, Reward bound : -0.7062997820743604\n",
      "iter_no ; 64, loss : 5.04646824562384e-16, Reward Mean : -3.078543917359241, Reward bound : -0.8508915020145552\n",
      "iter_no ; 65, loss : 1.2111524001255452e-15, Reward Mean : -3.805757874084978, Reward bound : -0.9364181051622342\n",
      "iter_no ; 66, loss : 5.04646824562384e-16, Reward Mean : -3.2036533920940022, Reward bound : -0.9551513855225455\n",
      "iter_no ; 67, loss : 7.266914294874153e-16, Reward Mean : -4.256471815517327, Reward bound : -1.5907609830616063\n",
      "iter_no ; 68, loss : 9.689219412762598e-16, Reward Mean : -4.312953047844566, Reward bound : -2.477294737080536\n",
      "iter_no ; 69, loss : 8.478066589120579e-16, Reward Mean : -3.5899521337761406, Reward bound : -0.9866370630384259\n",
      "iter_no ; 70, loss : 1.2918959217016798e-15, Reward Mean : -3.9239399906418173, Reward bound : -2.0806214584136757\n",
      "iter_no ; 71, loss : 1.2515241079740533e-15, Reward Mean : -2.5864122563424266, Reward bound : -0.5241039998706467\n",
      "iter_no ; 72, loss : 4.844609706381299e-16, Reward Mean : -4.092039988145038, Reward bound : -1.2866075119944986\n",
      "iter_no ; 73, loss : 6.661338147750939e-16, Reward Mean : -3.3732826082987337, Reward bound : -1.6967396739002507\n",
      "iter_no ; 74, loss : 3.1489961767989464e-15, Reward Mean : -3.1121248530681944, Reward bound : -0.8541217852090704\n",
      "iter_no ; 75, loss : 1.1304088785494106e-15, Reward Mean : -2.989874571799441, Reward bound : -0.5726791345243021\n",
      "iter_no ; 76, loss : 1.4533828589748305e-15, Reward Mean : -3.12249154086214, Reward bound : -0.7047659767876389\n",
      "iter_no ; 77, loss : 6.661338147750939e-16, Reward Mean : -2.7786867156912987, Reward bound : -0.793663603361583\n",
      "iter_no ; 78, loss : 8.074349510635498e-16, Reward Mean : -3.4203810146588074, Reward bound : -0.56749733518352\n",
      "iter_no ; 79, loss : 7.266914294874153e-16, Reward Mean : -4.697021348079536, Reward bound : -3.0371373264344794\n",
      "iter_no ; 80, loss : 2.4424906541753444e-15, Reward Mean : -3.430911558601202, Reward bound : -0.9547438663630183\n",
      "iter_no ; 81, loss : 6.459479608508399e-16, Reward Mean : -5.022270700210611, Reward bound : -2.5030655076520665\n",
      "iter_no ; 82, loss : 3.2297398042541994e-16, Reward Mean : -4.121416000658888, Reward bound : -1.4686663528919075\n",
      "iter_no ; 83, loss : 6.661338147750939e-16, Reward Mean : -2.1722649030600336, Reward bound : -0.2238157890794119\n",
      "iter_no ; 84, loss : 6.459479608508399e-16, Reward Mean : -4.078241511121512, Reward bound : -2.6698755166167194\n",
      "iter_no ; 85, loss : 5.04646824562384e-16, Reward Mean : -4.242079037612903, Reward bound : -2.075736619205943\n",
      "iter_no ; 86, loss : 8.881784197001252e-16, Reward Mean : -3.5347216399410533, Reward bound : -1.3718512204105204\n",
      "iter_no ; 87, loss : 1.71579917174837e-15, Reward Mean : -3.3272447751722956, Reward bound : -1.7359216914772764\n",
      "iter_no ; 88, loss : 1.9378438825525196e-15, Reward Mean : -3.945025501446417, Reward bound : -1.9674698851707415\n",
      "iter_no ; 89, loss : 8.074349510635498e-16, Reward Mean : -2.2038467205090164, Reward bound : -0.40251893003275996\n",
      "iter_no ; 90, loss : 1.6350557560513537e-15, Reward Mean : -2.684651857396874, Reward bound : -0.3079636246969202\n",
      "iter_no ; 91, loss : 1.6148699021270997e-15, Reward Mean : -3.719312416201975, Reward bound : -0.299123153821619\n",
      "iter_no ; 92, loss : 2.2810036110230752e-15, Reward Mean : -3.7392666125111615, Reward bound : -0.6564059201540173\n",
      "iter_no ; 93, loss : 1.513940526626711e-15, Reward Mean : -3.202423930146039, Reward bound : -1.5930229524559938\n",
      "iter_no ; 94, loss : 4.2390332945602897e-16, Reward Mean : -3.3051401990985703, Reward bound : -1.0692647129925352\n",
      "iter_no ; 95, loss : 2.341561384554074e-15, Reward Mean : -3.830875427211014, Reward bound : -1.0220710384817686\n",
      "iter_no ; 96, loss : 8.074349510635498e-16, Reward Mean : -2.5403071118042595, Reward bound : -0.7585568884678318\n",
      "iter_no ; 97, loss : 1.9378438825525196e-15, Reward Mean : -4.222723847004124, Reward bound : -2.029278804430488\n",
      "iter_no ; 98, loss : 1.6956133178241159e-15, Reward Mean : -3.715480793486097, Reward bound : -1.1211313475943596\n",
      "iter_no ; 99, loss : 6.661338147750939e-16, Reward Mean : -1.543994274205557, Reward bound : -0.3311099903916692\n",
      "iter_no ; 100, loss : 2.099330713946552e-15, Reward Mean : -3.2423822066676005, Reward bound : -0.3409376876894478\n",
      "iter_no ; 101, loss : 2.099330713946552e-15, Reward Mean : -3.7072410600684, Reward bound : -1.3587477780470794\n",
      "iter_no ; 102, loss : 1.049665356973276e-15, Reward Mean : -2.9760005241542196, Reward bound : -0.4013842906147679\n",
      "iter_no ; 103, loss : 8.881784197001252e-16, Reward Mean : -3.511683949746389, Reward bound : -2.307007807150571\n",
      "iter_no ; 104, loss : 9.285501275486333e-16, Reward Mean : -3.2386728622037624, Reward bound : -0.6998789309585027\n",
      "iter_no ; 105, loss : 8.074349510635498e-17, Reward Mean : -3.3641084135535726, Reward bound : -0.9824669972644442\n",
      "iter_no ; 106, loss : 2.0185873776588746e-17, Reward Mean : -3.14491423513094, Reward bound : -0.9539662315804065\n",
      "iter_no ; 107, loss : 1.4533828589748305e-15, Reward Mean : -2.04221538207991, Reward bound : -0.6074870564177803\n",
      "iter_no ; 108, loss : 6.459479608508399e-16, Reward Mean : -3.0256901262191445, Reward bound : -1.1710643932244993\n",
      "iter_no ; 109, loss : 2.03877315217379e-15, Reward Mean : -3.7023251097663925, Reward bound : -0.9156741270082863\n",
      "iter_no ; 110, loss : 4.037174755317749e-16, Reward Mean : -4.0069216766630635, Reward bound : -1.1483040636560962\n",
      "iter_no ; 111, loss : 6.459479608508399e-16, Reward Mean : -4.115359729599808, Reward bound : -2.2805991256217792\n",
      "iter_no ; 112, loss : 1.2313382540497992e-15, Reward Mean : -3.9674580328254816, Reward bound : -1.4132502604469968\n",
      "iter_no ; 113, loss : 1.2918959217016798e-15, Reward Mean : -4.716452752986557, Reward bound : -1.4487904690762314\n",
      "iter_no ; 114, loss : 1.4735687128990846e-15, Reward Mean : -4.209586370991371, Reward bound : -1.3081177542335383\n",
      "iter_no ; 115, loss : 9.689219412762598e-16, Reward Mean : -2.9324917163490394, Reward bound : -0.8691831735411307\n",
      "iter_no ; 116, loss : 1.9378438825525196e-15, Reward Mean : -3.5757335907998637, Reward bound : -1.5835316647335171\n",
      "iter_no ; 117, loss : 1.2111524001255452e-15, Reward Mean : -3.6882464060510167, Reward bound : -1.0388835872177693\n",
      "iter_no ; 118, loss : 1.2111524001255452e-15, Reward Mean : -3.348707581143768, Reward bound : -1.0985050025791372\n",
      "iter_no ; 119, loss : 8.276208049878039e-16, Reward Mean : -2.873064662164337, Reward bound : -0.8215595138412397\n",
      "iter_no ; 120, loss : 2.099330713946552e-15, Reward Mean : -4.121716009267543, Reward bound : -2.035838192895861\n",
      "iter_no ; 121, loss : 4.037174755317749e-16, Reward Mean : -3.3878817208681298, Reward bound : -1.8307903754387942\n",
      "iter_no ; 122, loss : 1.6148699021270997e-15, Reward Mean : -3.3105397098482543, Reward bound : -1.483799169501196\n",
      "iter_no ; 123, loss : 1.049665356973276e-15, Reward Mean : -3.8235144889228265, Reward bound : -2.141805305926273\n",
      "iter_no ; 124, loss : 9.689219412762598e-16, Reward Mean : -3.837007320247669, Reward bound : -1.1887225761577649\n",
      "iter_no ; 125, loss : 1.1505947324736646e-15, Reward Mean : -3.1369168183352896, Reward bound : -0.8766903450372885\n",
      "iter_no ; 126, loss : 6.661338147750939e-16, Reward Mean : -3.0014676474931488, Reward bound : -1.7820845158741299\n",
      "iter_no ; 127, loss : 1.049665356973276e-15, Reward Mean : -3.7682138196811117, Reward bound : -1.0343726613141857\n",
      "iter_no ; 128, loss : 9.083642736243793e-16, Reward Mean : -2.7731712421399664, Reward bound : -0.6027893791992714\n",
      "iter_no ; 129, loss : 3.43159834349674e-16, Reward Mean : -4.894144048992859, Reward bound : -2.182397106858893\n",
      "iter_no ; 130, loss : 2.5837918434033595e-15, Reward Mean : -3.3337784413012543, Reward bound : -0.38471508255434966\n",
      "iter_no ; 131, loss : 8.074349510635498e-16, Reward Mean : -2.613229890609327, Reward bound : -1.038426874865615\n",
      "iter_no ; 132, loss : 4.037174755317749e-16, Reward Mean : -3.1285914332036815, Reward bound : -0.36940370246652277\n",
      "iter_no ; 133, loss : 5.04646824562384e-16, Reward Mean : -2.950414459981749, Reward bound : -0.7231649362288606\n",
      "iter_no ; 134, loss : 1.0698513167766485e-15, Reward Mean : -2.9494282676932033, Reward bound : -0.8542355596539892\n",
      "iter_no ; 135, loss : 8.074349510635498e-16, Reward Mean : -3.318137215054486, Reward bound : -0.5846709446085371\n",
      "iter_no ; 136, loss : 1.049665356973276e-15, Reward Mean : -3.1533208409822664, Reward bound : -1.9230500842183362\n",
      "iter_no ; 137, loss : 4.844609706381299e-16, Reward Mean : -2.4080605450787074, Reward bound : -1.1363829843931583\n",
      "iter_no ; 138, loss : 9.083642736243793e-16, Reward Mean : -2.8513427575884718, Reward bound : -1.2443072299205893\n",
      "iter_no ; 139, loss : 2.4223048002510903e-15, Reward Mean : -3.622138381637323, Reward bound : -2.048715502804235\n",
      "iter_no ; 140, loss : 1.6956133178241159e-15, Reward Mean : -3.6628578121644044, Reward bound : -1.0013122810101183\n",
      "iter_no ; 141, loss : 1.2111524001255452e-15, Reward Mean : -2.8373976334381297, Reward bound : -0.07618881429309511\n",
      "iter_no ; 142, loss : 9.689219412762598e-16, Reward Mean : -2.0161173422408707, Reward bound : -0.8176078758996081\n",
      "iter_no ; 143, loss : 2.6645352591003757e-15, Reward Mean : -3.2035634670487605, Reward bound : -0.6526735071610965\n",
      "iter_no ; 144, loss : 8.881784197001252e-16, Reward Mean : -2.6735534458128773, Reward bound : -1.2191163684804245\n",
      "iter_no ; 145, loss : 1.1304088785494106e-15, Reward Mean : -4.008382420384552, Reward bound : -1.6537739371767783\n",
      "iter_no ; 146, loss : 7.266914294874153e-16, Reward Mean : -2.986946140325232, Reward bound : -2.8274163481481143\n",
      "iter_no ; 147, loss : 2.341561384554074e-15, Reward Mean : -3.184219902799061, Reward bound : -2.0845923074078314\n",
      "iter_no ; 148, loss : 1.7460780585138694e-15, Reward Mean : -2.22988804448243, Reward bound : -0.70387142233237\n",
      "iter_no ; 149, loss : 8.276208049878039e-16, Reward Mean : -2.8823246444030173, Reward bound : -1.2557170643430426\n",
      "iter_no ; 150, loss : 7.266914294874153e-16, Reward Mean : -4.56396772255402, Reward bound : -1.578454816726909\n",
      "iter_no ; 151, loss : 1.6148699021270997e-15, Reward Mean : -3.967955605022456, Reward bound : -1.5256136318417122\n",
      "iter_no ; 152, loss : 6.459479608508399e-16, Reward Mean : -3.3983111433425126, Reward bound : -0.45737445946338473\n",
      "iter_no ; 153, loss : 3.43159834349674e-16, Reward Mean : -3.395711251788256, Reward bound : -1.2250279549859289\n",
      "iter_no ; 154, loss : 2.180074341401805e-15, Reward Mean : -2.621200860400501, Reward bound : -1.0463105737487868\n",
      "iter_no ; 155, loss : 1.2313382540497992e-15, Reward Mean : -2.6570242921374705, Reward bound : -0.3153620276190216\n",
      "iter_no ; 156, loss : 2.341561384554074e-15, Reward Mean : -3.426011565211205, Reward bound : -2.038233554267343\n",
      "iter_no ; 157, loss : 4.844609706381299e-16, Reward Mean : -3.8363467135074796, Reward bound : -1.4924037542491169\n",
      "iter_no ; 158, loss : 1.7965426933245045e-15, Reward Mean : -3.1625322130431015, Reward bound : -1.4866580992235154\n",
      "iter_no ; 159, loss : 1.009293649124768e-15, Reward Mean : -2.7296193178264354, Reward bound : -1.7289682594169204\n",
      "iter_no ; 160, loss : 2.5232340698723606e-15, Reward Mean : -2.980558604717073, Reward bound : -0.8754650785344262\n",
      "iter_no ; 161, loss : 1.3928252972020684e-15, Reward Mean : -3.3616445493872114, Reward bound : -0.9656702120639553\n",
      "iter_no ; 162, loss : 5.24832678486638e-16, Reward Mean : -3.623965713051035, Reward bound : -1.2123125951286875\n",
      "iter_no ; 163, loss : 8.074349510635498e-17, Reward Mean : -3.831007712577315, Reward bound : -2.4237962373754467\n",
      "iter_no ; 164, loss : 1.049665356973276e-15, Reward Mean : -3.4888546572094956, Reward bound : -0.9746922709982315\n",
      "iter_no ; 165, loss : 1.5543122344752192e-15, Reward Mean : -3.6919453551636003, Reward bound : -1.4955744782858749\n",
      "iter_no ; 166, loss : 1.1505947324736646e-15, Reward Mean : -3.4619164628332406, Reward bound : -1.5599854127585981\n",
      "iter_no ; 167, loss : 2.4223048002510903e-15, Reward Mean : -2.40817240938107, Reward bound : -0.36168483817541497\n",
      "iter_no ; 168, loss : 1.8369144011730126e-15, Reward Mean : -2.856080851326184, Reward bound : -0.7169751398353228\n",
      "iter_no ; 169, loss : 1.8167285737185382e-16, Reward Mean : -2.07952874874269, Reward bound : -0.7696908327242801\n",
      "iter_no ; 170, loss : 2.084191482322039e-15, Reward Mean : -3.619424517204708, Reward bound : -1.840662935377762\n",
      "iter_no ; 171, loss : 8.276208049878039e-16, Reward Mean : -3.9490042103148584, Reward bound : -2.109462412819827\n",
      "iter_no ; 172, loss : 3.4719702631034847e-15, Reward Mean : -2.9688430602713813, Reward bound : -0.8373350279075693\n",
      "iter_no ; 173, loss : 9.689219412762598e-16, Reward Mean : -4.353003972330826, Reward bound : -2.6162731155923424\n",
      "iter_no ; 174, loss : 5.652044392747053e-16, Reward Mean : -3.5013906646862836, Reward bound : -1.1677605310555528\n",
      "iter_no ; 175, loss : 2.745278674797392e-15, Reward Mean : -3.890315268326724, Reward bound : -1.9050252170987205\n",
      "iter_no ; 176, loss : 3.2297398042541994e-16, Reward Mean : -4.646003463863442, Reward bound : -2.848991259534653\n",
      "iter_no ; 177, loss : 2.099330713946552e-15, Reward Mean : -3.015082379793835, Reward bound : -1.3794740224187394\n",
      "iter_no ; 178, loss : 6.055762000627726e-16, Reward Mean : -3.3185014395722368, Reward bound : -0.8733441575186889\n",
      "iter_no ; 179, loss : 9.891077952005139e-16, Reward Mean : -4.530748843295129, Reward bound : -1.9641972256341627\n",
      "iter_no ; 180, loss : 1.8571002550972666e-15, Reward Mean : -2.9312721509770956, Reward bound : -0.8078881612198938\n",
      "iter_no ; 181, loss : 1.2111524001255452e-15, Reward Mean : -3.3435582912443413, Reward bound : -1.0995447275711874\n",
      "iter_no ; 182, loss : 9.689219412762598e-16, Reward Mean : -1.879556470715564, Reward bound : -0.12683257730794784\n",
      "iter_no ; 183, loss : 8.074349510635498e-16, Reward Mean : -2.9990971249778204, Reward bound : -0.6641342811590432\n",
      "iter_no ; 184, loss : 1.049665356973276e-15, Reward Mean : -4.711526608859339, Reward bound : -2.2286760331789943\n",
      "iter_no ; 185, loss : 1.1304088785494106e-15, Reward Mean : -3.1820303043351204, Reward bound : -1.1691941347207047\n",
      "iter_no ; 186, loss : 9.689219412762598e-16, Reward Mean : -4.8440237756690525, Reward bound : -2.985859282763966\n",
      "iter_no ; 187, loss : 1.6148699021270997e-15, Reward Mean : -4.595198115039567, Reward bound : -2.0321237464415827\n",
      "iter_no ; 188, loss : 2.200260195326059e-15, Reward Mean : -1.5450503025174673, Reward bound : -0.3322973772502789\n",
      "iter_no ; 189, loss : 0.0, Reward Mean : -4.114252719676422, Reward bound : -1.4888374584553392\n",
      "iter_no ; 190, loss : 1.2918959217016798e-15, Reward Mean : -5.082916837839716, Reward bound : -3.92635492909472\n",
      "iter_no ; 191, loss : 5.298791949072607e-16, Reward Mean : -3.440199386599494, Reward bound : -1.7002549541653385\n",
      "iter_no ; 192, loss : 2.2810036110230752e-15, Reward Mean : -2.503926562040365, Reward bound : -1.4907486911325667\n",
      "iter_no ; 193, loss : 1.6148699021270997e-16, Reward Mean : -3.5170124734776866, Reward bound : -1.9513425426272621\n",
      "iter_no ; 194, loss : 9.083642736243793e-16, Reward Mean : -2.3292190340848764, Reward bound : -0.3713835897200539\n",
      "iter_no ; 195, loss : 8.276208049878039e-16, Reward Mean : -3.535060726410375, Reward bound : -0.5876293364698837\n",
      "iter_no ; 196, loss : 4.037174755317749e-16, Reward Mean : -4.1867558357603825, Reward bound : -2.2963479229857375\n",
      "iter_no ; 197, loss : 2.099330713946552e-15, Reward Mean : -3.3753440213475985, Reward bound : -1.5433412611734085\n",
      "iter_no ; 198, loss : 4.844609706381299e-16, Reward Mean : -1.1724473363362942, Reward bound : -0.1880732196924379\n",
      "solved\n"
     ]
    }
   ],
   "source": [
    "# Training Agent \n",
    "HIDDEN_LAYER = 150\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = collections.namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = collections.namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiate Environment\n",
    "env = gym.make('Pendulum-v1', render_mode = 'human')\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions_ = env.action_space.shape[0]\n",
    "\n",
    "# Intiate Neural Network , Loss Function and Optimizer\n",
    "Net = PolicyNeuralNetwork(obs_size=obs_size, hidden_size=HIDDEN_LAYER, n_actions=n_actions_)\n",
    "objective = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=Net.parameters(), lr=0.001)\n",
    "\n",
    "# Training NN and Agent \n",
    "for iter_no, batch in enumerate(iterate_batches(env=env, Policy=Net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying Optimization \n",
    "    obs_vector, action_vector, reward_boundary, reward_mean = filter_batches(batches = batch, percentile=PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_score_vector = Net(obs_vector) # trainining only on the best\n",
    "\n",
    "    loss_vector = objective(action_score_vector, action_vector)\n",
    "    loss_vector.backward() # back propagation\n",
    "    optimizer.step() # applying and update \n",
    "    time.sleep(1/10)\n",
    "    env.render()\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_vector.item()}, Reward Mean : {reward_mean}, Reward bound : {reward_boundary}\")\n",
    "\n",
    "    if reward_mean > -1.5:\n",
    "        print(\"solved\")\n",
    "        break\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ Solving \\ the \\ Mountain \\ Car \\ Problem \\ with \\ Cross \\ Entropy \\ - \\ Functions $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\" Internal NN Policy mapper decision for the agent to choose an action \"\"\"\n",
    "    def __init__(self, obs_size:int, hidden_size:int, n_actions:int):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(in_features=obs_size, out_features=hidden_size),\n",
    "            nn.LeakyReLU (),\n",
    "            nn.Linear(in_features=hidden_size, out_features=550),\n",
    "            nn.LeakyReLU (),\n",
    "            nn.Linear(in_features=550, out_features=n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, X:torch.FloatTensor)-> torch.FloatTensor:\n",
    "        return self.policy(X)\n",
    "    \n",
    "\n",
    "def batches_iterator (env:gym.make, AgentPolicy: nn.Module, batch_size:int):\n",
    "    \"\"\"\n",
    "    Iterate through batches of episodes.\n",
    "\n",
    "    Args:\n",
    "        env: The gym environment to use.\n",
    "        policy_network: The neural network that defines the agent's policy.\n",
    "        batch_size: The number of episodes per batch.\n",
    "\n",
    "    Yields:\n",
    "        A batch of episodes, represented as a list of Episode objects.\n",
    "    \"\"\"\n",
    "\n",
    "    batch , episode_step = [], []\n",
    "    episode_reward, step_count = 0.0 , 0\n",
    "    obs = env.reset()[0] # the new version return the output as tuple \n",
    "    activation_function = nn.Softmax(dim=1) # activation function \n",
    "\n",
    "    while True:\n",
    "        obs_vector = torch.FloatTensor(np.array([obs]))\n",
    "        act_prob_vector = activation_function(AgentPolicy(obs_vector)) # get the output probabilities from Neural Network\n",
    "        act_prob = act_prob_vector.detach().numpy()[0] \n",
    "        action = np.random.choice(len(act_prob), p= act_prob)\n",
    "\n",
    "        # Environment\n",
    "        next_obs, rewards , is_done, _, _ = env.step(action)\n",
    "        episode_reward += rewards * 0.95\n",
    "\n",
    "        # Track the best steps \n",
    "        episode_step.append(EpisodeStep(observation = obs, action = action))\n",
    "        \n",
    "        # Reset\n",
    "        if is_done or step_count > EPISODES:\n",
    "            batch.append(Episode(reward=episode_reward, steps = episode_step))\n",
    "            episode_reward = 0.0 \n",
    "            next_obs = env.reset()[0]\n",
    "            episode_step = []\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs\n",
    "        step_count += 1 \n",
    "\n",
    "def filter_batches (batch:namedtuple, percentile:int)-> list:\n",
    "    \"\"\" This function filter the elite or best Episode to retrain the NN\n",
    "    Parameters:\n",
    "    - batch: namedtuple, containing the Rewards and Steps\n",
    "    - percentile: int, percentile to filter the batches \"\"\"\n",
    "\n",
    "    # Filtering Rewards\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # Get those reward that are above the percentile \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs, train_act = [], []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: # Filter the batches which reward is above the rewards_bound\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step:step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ Mountain \\ Car $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_no ; 0, loss : 1.0993001461029053, Reward Mean : -2.078125, Reward bound : -0.5\n",
      "iter_no ; 1, loss : 2.868468527594814e-06, Reward Mean : -0.5, Reward bound : -0.5\n",
      "iter_no ; 2, loss : 0.0, Reward Mean : -0.5, Reward bound : -0.5\n",
      "iter_no ; 3, loss : 0.0, Reward Mean : -0.5, Reward bound : -0.5\n",
      "iter_no ; 4, loss : 0.0, Reward Mean : -0.5, Reward bound : -0.5\n",
      "iter_no ; 5, loss : 0.0, Reward Mean : -0.5, Reward bound : -0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Chapter_4_practice.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(comment\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-mountain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Start Training \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m iter_no, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batches_iterator(env\u001b[39m=\u001b[39menv, AgentPolicy\u001b[39m=\u001b[39mnet, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Applying optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     obs_v, acts_v, reward_b, reward_m \u001b[39m=\u001b[39m filter_batches(batch\u001b[39m=\u001b[39mbatch, percentile\u001b[39m=\u001b[39mPERCENTILE) \u001b[39m# Take the best Scenarios\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Reset the Gradient \u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Chapter_4_practice.ipynb Cell 10\u001b[0m in \u001b[0;36mbatches_iterator\u001b[1;34m(env, AgentPolicy, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m batch\u001b[39m.\u001b[39mappend(Episode(reward\u001b[39m=\u001b[39mepisode_reward, steps \u001b[39m=\u001b[39m episode_step))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m episode_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m next_obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m episode_step \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m batch_size:\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:164\u001b[0m, in \u001b[0;36mMountainCarEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnp_random\u001b[39m.\u001b[39muniform(low\u001b[39m=\u001b[39mlow, high\u001b[39m=\u001b[39mhigh), \u001b[39m0\u001b[39m])\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), {}\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    267\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the Neural Network \n",
    "HIDDEN_LAYERS = 400\n",
    "BATCH_SIZE = 32\n",
    "PERCENTILE = 90\n",
    "EPISODES = 100\n",
    "\n",
    "# Keep tracking of each episode and Steps\n",
    "Episode = namedtuple('Episode', field_names= ['reward','steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "\n",
    "# Initiating Environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode = 'human')\n",
    "obs_size_ = env.observation_space.shape[0] # number of output in the environment ->  ndarray with shape (1,) which takes values {0,1} where 0, push cart to the left, and 1, push cart to the right  \n",
    "n_actions_ = env.action_space.n  #accelerate , stop, dont accelerate\n",
    "\n",
    "# Initiate Neural Network , Loss Functions and Optimizer\n",
    "net = NeuralNetwork(obs_size = obs_size_, hidden_size= HIDDEN_LAYERS, n_actions = n_actions_)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.1)\n",
    "writer = SummaryWriter(comment=\"-mountain\")\n",
    "\n",
    "# Start Training \n",
    "for iter_no, batch in enumerate(batches_iterator(env=env, AgentPolicy=net, batch_size=BATCH_SIZE)):\n",
    "\n",
    "    # Applying optimization\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batches(batch=batch, percentile=PERCENTILE) # Take the best Scenarios\n",
    "    optimizer.zero_grad() # Reset the Gradient \n",
    "    action_score_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_score_v, acts_v) # compare the action output vs the winning action \n",
    "    loss_v.backward()\n",
    "    optimizer.step() # Apply back Propagation\n",
    "\n",
    "    print(f\"iter_no ; {iter_no}, loss : {loss_v.item()}, Reward Mean : {reward_m}, Reward bound : {reward_b}\")\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m >= -.10:\n",
    "        print(reward_m)\n",
    "        print(\"solved!\")\n",
    "        break \n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Chapter_4_practice.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     env\u001b[39m.\u001b[39;49mstep(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Chapter_4_practice.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     env\u001b[39m.\u001b[39mstep(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m (position, velocity)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    267\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode = 'human')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.step(2)\n",
    "    env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Playing Random Games\n",
      "episode = 0, Reward=1880\n",
      "episode = 1, Reward=0\n",
      "episode = 2, Reward=0\n",
      "episode = 3, Reward=0\n",
      "episode = 4, Reward=0\n",
      "episode = 5, Reward=0\n",
      "episode = 6, Reward=0\n",
      "episode = 7, Reward=0\n",
      "episode = 8, Reward=0\n",
      "episode = 9, Reward=0\n",
      "episode = 10, Reward=0\n",
      "episode = 11, Reward=0\n",
      "episode = 12, Reward=0\n",
      "episode = 13, Reward=0\n",
      "episode = 14, Reward=0\n",
      "episode = 15, Reward=0\n",
      "episode = 16, Reward=0\n",
      "episode = 17, Reward=0\n",
      "episode = 18, Reward=0\n",
      "episode = 19, Reward=0\n",
      "episode = 20, Reward=0\n",
      "episode = 21, Reward=0\n",
      "episode = 22, Reward=0\n",
      "episode = 23, Reward=0\n",
      "episode = 24, Reward=0\n",
      "episode = 25, Reward=0\n",
      "episode = 26, Reward=0\n",
      "episode = 27, Reward=0\n",
      "episode = 28, Reward=0\n",
      "episode = 29, Reward=0\n",
      "episode = 30, Reward=0\n",
      "episode = 31, Reward=0\n",
      "episode = 32, Reward=0\n",
      "episode = 33, Reward=0\n",
      "episode = 34, Reward=0\n",
      "episode = 35, Reward=0\n",
      "episode = 36, Reward=0\n",
      "episode = 37, Reward=0\n",
      "episode = 38, Reward=0\n",
      "episode = 39, Reward=0\n",
      "episode = 40, Reward=0\n",
      "episode = 41, Reward=0\n",
      "episode = 42, Reward=0\n",
      "episode = 43, Reward=0\n",
      "episode = 44, Reward=0\n",
      "episode = 45, Reward=0\n",
      "episode = 46, Reward=0\n",
      "episode = 47, Reward=0\n",
      "episode = 48, Reward=0\n",
      "episode = 49, Reward=0\n",
      "Epoch 1/80\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 1.0991\n",
      "Epoch 2/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0993\n",
      "Epoch 3/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0985\n",
      "Epoch 4/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0984\n",
      "Epoch 5/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0985\n",
      "Epoch 6/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0985\n",
      "Epoch 7/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0985\n",
      "Epoch 8/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0985\n",
      "Epoch 9/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0982\n",
      "Epoch 10/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0982\n",
      "Epoch 11/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0982\n",
      "Epoch 12/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0983\n",
      "Epoch 13/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 14/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0982\n",
      "Epoch 15/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0981\n",
      "Epoch 16/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0982\n",
      "Epoch 17/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0981\n",
      "Epoch 18/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 19/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0983\n",
      "Epoch 20/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0982\n",
      "Epoch 21/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0982\n",
      "Epoch 22/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0981\n",
      "Epoch 23/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0980\n",
      "Epoch 24/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0980\n",
      "Epoch 25/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0981\n",
      "Epoch 26/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 27/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 28/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 29/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 30/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 31/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 32/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 33/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0979\n",
      "Epoch 34/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0980\n",
      "Epoch 35/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0979\n",
      "Epoch 36/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 37/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 38/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0979\n",
      "Epoch 39/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 40/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 41/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 42/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 43/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0978\n",
      "Epoch 44/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 45/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 46/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0977\n",
      "Epoch 47/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 48/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0977\n",
      "Epoch 49/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0977\n",
      "Epoch 50/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 51/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 52/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0978\n",
      "Epoch 53/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 54/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0979\n",
      "Epoch 55/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0976\n",
      "Epoch 56/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 57/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 58/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 59/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0976\n",
      "Epoch 60/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 61/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0976\n",
      "Epoch 62/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0977\n",
      "Epoch 63/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0976\n",
      "Epoch 64/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 65/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0975\n",
      "Epoch 66/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0978\n",
      "Epoch 67/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0977\n",
      "Epoch 68/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0976\n",
      "Epoch 69/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0975\n",
      "Epoch 70/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0976\n",
      "Epoch 71/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0975\n",
      "Epoch 72/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0973\n",
      "Epoch 73/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0978\n",
      "Epoch 74/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0974\n",
      "Epoch 75/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0976\n",
      "Epoch 76/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0974\n",
      "Epoch 77/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0974\n",
      "Epoch 78/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0976\n",
      "Epoch 79/80\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1.0973\n",
      "Epoch 80/80\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 1.0975\n",
      "[+] Playing Games with NN\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Practice_4_1_practice.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 204>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39m# Initialize Game Environment\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mMountainCar-v0\u001b[39m\u001b[39m'\u001b[39m, render_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m play_game(ml_model\u001b[39m=\u001b[39;49mml_model, games\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Practice_4_1_practice.ipynb Cell 12\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(ml_model, games)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     \u001b[39m# render = env.render()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     \u001b[39m# Predict Next Movement\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     current_action_pred \u001b[39m=\u001b[39m ml_model\u001b[39m.\u001b[39mpredict(observation\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m     \u001b[39m# Define Movement\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X14sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m     current_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(current_action_pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Playing Random Games\n",
      "Reward=11026\n",
      "Reward=1381\n",
      "Reward=108\n",
      "Reward=11518\n",
      "Reward=6968\n",
      "Reward=7287\n",
      "Reward=7754\n",
      "Reward=236\n",
      "[+] Training NN Model\n",
      "Epoch 1/80\n",
      "7515/7515 [==============================] - 10s 1ms/step - loss: 1.0987\n",
      "Epoch 2/80\n",
      "7515/7515 [==============================] - 12s 2ms/step - loss: 1.0987\n",
      "Epoch 3/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 4/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 5/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 6/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 7/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 8/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 9/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 10/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 11/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 12/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0986\n",
      "Epoch 13/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 14/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 15/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 16/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 17/80\n",
      "7515/7515 [==============================] - 11s 1ms/step - loss: 1.0987\n",
      "Epoch 18/80\n",
      "  57/7515 [..............................] - ETA: 13s - loss: 1.0991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Practice_4_1_practice.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 198>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m df \u001b[39m=\u001b[39m play_random_games(games\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[+] Training NN Model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m ml_model \u001b[39m=\u001b[39m generate_ml(df)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[+] Playing Games with NN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m play_game(ml_model\u001b[39m=\u001b[39mml_model, games\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\11109808\\OneDrive - Wistron Corporation\\Desktop\\Reinforcement Learning\\Practice_4_1_practice.ipynb Cell 14\u001b[0m in \u001b[0;36mgenerate_ml\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# Fit Model with Data\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     dataframe[[\u001b[39m'\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvelocity\u001b[39;49m\u001b[39m'\u001b[39;49m]],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     dataframe[[\u001b[39m'\u001b[39;49m\u001b[39maction_0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maction_1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maction_2\u001b[39;49m\u001b[39m'\u001b[39;49m]],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/11109808/OneDrive%20-%20Wistron%20Corporation/Desktop/Reinforcement%20Learning/Practice_4_1_practice.ipynb#X16sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\11109808\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import namedtuple\n",
    "from time import sleep\n",
    "\n",
    "RIGHT_CMD = [0, 1]\n",
    "LEFT_CMD = [1, 0]\n",
    "\n",
    "# Define Reward Config\n",
    "BEST_GAMES_TO_EVOLVE = 10\n",
    "\n",
    "# Define Game Commands\n",
    "GAME_ACTIONS_MAPPING_TO_ARRAY = [\n",
    "    [1, 0, 0],  # Movement 0\n",
    "    [0, 1, 0],  # Movement 1\n",
    "    [0, 0, 1]   # Movement 2\n",
    "]\n",
    "\n",
    "# Initialize Game Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Define Structures\n",
    "GameData = namedtuple('GameData', field_names = ['reward','data'])\n",
    "\n",
    "\n",
    "def compute_reward(position):\n",
    "    \"\"\"\n",
    "    Compute Reward for Current Position.\n",
    "    :param position:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Update Best Position\n",
    "    if position >= -0.1000000:\n",
    "        return 6\n",
    "    if position >= -0.1100000:\n",
    "        return 5\n",
    "    if position >= -0.1300000:\n",
    "        return 4\n",
    "    if position >= -0.1500000:\n",
    "        return 3\n",
    "    if position >= -0.1700000:\n",
    "        return 2\n",
    "    if position >= -0.2000000:\n",
    "        return 1\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def play_random_games(games=100):\n",
    "    \"\"\"\n",
    "    Play Random Games to Get Some Observations\n",
    "    :param games:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Storage for All Games Movements\n",
    "    all_movements = []\n",
    "\n",
    "    for episode in range(games):\n",
    "\n",
    "        # Reset Game Reward\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Define Storage for Current Game Data\n",
    "        current_game_data = []\n",
    "\n",
    "        # Reset Game Environment\n",
    "        env.reset()\n",
    "\n",
    "        # Get First Random Movement\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Play\n",
    "            observation, reward, done, info, _ = env.step(action)  # observation=position, velocity\n",
    "\n",
    "            # Update Reward Value\n",
    "            reward = compute_reward(observation[[0]])\n",
    "\n",
    "            # Get Random Action (On Real, its get a \"Next\" movement to compensate Previous Movement)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Store Observation Data and Action Taken\n",
    "            current_game_data.append(\n",
    "                np.hstack((observation, GAME_ACTIONS_MAPPING_TO_ARRAY[action]))\n",
    "            )\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Compute Reward\n",
    "        if episode_reward > -199.0:\n",
    "            print(f'Reward={episode_reward}')\n",
    "\n",
    "            # Save All Data\n",
    "            all_movements.append(\n",
    "                GameData(episode_reward, current_game_data)\n",
    "            )\n",
    "\n",
    "    # Sort Movements Array\n",
    "    all_movements.sort(key=lambda item: item.reward, reverse=True)\n",
    "\n",
    "    # Filter the best N games\n",
    "    all_movements = all_movements[BEST_GAMES_TO_EVOLVE] if len(all_movements) > BEST_GAMES_TO_EVOLVE else all_movements\n",
    "\n",
    "    # Retrieve only the Game Movements\n",
    "    movements_only = []\n",
    "    for single_game_movements in all_movements:\n",
    "        movements_only.extend([item for item in single_game_movements.data])\n",
    "\n",
    "    # Create DataFrame\n",
    "    dataframe = pd.DataFrame(\n",
    "        movements_only,\n",
    "        columns=['position', 'velocity', 'action_0', 'action_1', 'action_2']\n",
    "    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def generate_ml(dataframe):\n",
    "    \"\"\"\n",
    "    Train and Generate NN Model\n",
    "    :param dataframe:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define Neural Network Topology\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "    # model.add(Dense(128,  activation='relu'))\n",
    "    # model.add(Dense(128,  activation='relu'))\n",
    "    model.add(Dense(64,  activation='relu'))\n",
    "    model.add(Dense(32,  activation='relu'))\n",
    "    model.add(Dense(3,  activation='sigmoid'))\n",
    "\n",
    "    # Compile Neural Network\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    # Fit Model with Data\n",
    "    model.fit(\n",
    "        dataframe[['position', 'velocity']],\n",
    "        dataframe[['action_0', 'action_1', 'action_2']],\n",
    "        epochs=80\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def play_game(ml_model, games=100):\n",
    "    \"\"\"\n",
    "    Play te Game\n",
    "    :param ml_model:\n",
    "    :param games:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for i_episode in range(games):\n",
    "\n",
    "        # Define Reward Var\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Reset Env for the Game\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            render = env.render()\n",
    "            sleep(0.01)\n",
    "\n",
    "            # Predict Next Movement\n",
    "            current_action_pred = ml_model.predict(observation.reshape(1, 2))[0]\n",
    "\n",
    "            # Define Movement\n",
    "            current_action = np.argmax(current_action_pred)\n",
    "\n",
    "            # Make Movement\n",
    "            observation, reward, done, info, _ = env.step(current_action)\n",
    "\n",
    "            # Update Reward Value\n",
    "            episode_reward += compute_reward(observation[[0]])\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode finished after {i_episode+1} steps\", end='')\n",
    "                break\n",
    "\n",
    "        print(f\" Score = {episode_reward}\")\n",
    "\n",
    "\n",
    "print(\"[+] Playing Random Games\")\n",
    "df = play_random_games(games=10)\n",
    "\n",
    "print(\"[+] Training NN Model\")\n",
    "ml_model = generate_ml(df)\n",
    "\n",
    "print(\"[+] Playing Games with NN\")\n",
    "play_game(ml_model=ml_model, games=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
