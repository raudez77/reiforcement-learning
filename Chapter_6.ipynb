{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  $ Tabular \\ Q-learning $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated : 0 -> 0.1\n",
      "Best reward updated : 0.1 -> 0.2\n",
      "Best reward updated : 0.2 -> 0.25\n",
      "Best reward updated : 0.25 -> 0.35\n",
      "Best reward updated : 0.35 -> 0.4\n",
      "Best reward updated : 0.4 -> 0.45\n",
      "Best reward updated : 0.45 -> 0.5\n",
      "Best reward updated : 0.5 -> 0.55\n",
      "Best reward updated : 0.55 -> 0.65\n",
      "Best reward updated : 0.65 -> 0.75\n",
      "Best reward updated : 0.75 -> 0.8\n",
      "Best reward updated : 0.8 -> 0.9\n",
      "Best reward updated : 0.9 -> 0.95\n",
      "Solved in 8019 iterations\n"
     ]
    }
   ],
   "source": [
    "class AgentClass:\n",
    "    \"\"\" Module that create a agent object and Initiate values \n",
    "    \n",
    "    Args:\n",
    "        - environment : Containing the Environment from gym \n",
    "        - alpha : float, learning rate for Q-Learning\n",
    "        - gamma : float, Penalty in Q-Learning\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_env :gym.make, alpha_:float, gamma_:float):\n",
    "        self.train_env = train_env\n",
    "        self.state = self.train_env.reset()\n",
    "        self.alpha_ = alpha_\n",
    "        self.gamma_ = gamma_\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        # Initiate Environment \n",
    "        action = self.train_env.action_space.sample()\n",
    "        old_state = self.state\n",
    "\n",
    "        new_state, reward, is_done, _ , _  = self.train_env.step(action)\n",
    "        self.state = self.train_env.reset() if is_done else new_state\n",
    "        return old_state, action, reward, new_state\n",
    "    \n",
    "    def best_value_and_action_on_test_environment (self,state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.train_env.action_space.n):\n",
    "            action_value = self.values[f\"{(state, action)}\"]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "    \n",
    "    def value_update (self, s, a, r, next_s):\n",
    "        \"\"\"\n",
    "        The next method receives the state of the environment and finds the best action to \n",
    "        take from this state by taking the action with the largest value that we have in the \n",
    "        table\n",
    "        \"\"\"\n",
    "\n",
    "        best_v, _ = self.best_value_and_action_on_test_environment(next_s)\n",
    "        new_v = r + self.gamma_ * best_v\n",
    "        old_v = self.values[f\"{(s,a)}\"]\n",
    "        self.values[f\"{(s,a)}\"] = old_v * (1-self.alpha_) + new_v * self.alpha_\n",
    "\n",
    "\n",
    "    def play_episode (self, test_env:gym.make):\n",
    "        \n",
    "        total_reward = 0.0 \n",
    "        state = test_env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action_on_test_environment(state)\n",
    "            new_state, reward, is_done, _ , _ = test_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "# Testing Environment \n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODE = 20 \n",
    "\n",
    "# Initiate \n",
    "train_env = gym.make(id = ENV_NAME)\n",
    "test_env = gym.make(id = ENV_NAME, render_mode = None)\n",
    "Agent = AgentClass(train_env=train_env, alpha_=ALPHA, gamma_=GAMMA)\n",
    "writer = SummaryWriter(comment = \"-FrozenLake-Q-Learning\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0 \n",
    "\n",
    "\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    state_, action_, reward_ , new_state_ = Agent.sample_env()\n",
    "    Agent.value_update(state_, action_, reward_, new_state_)\n",
    "    reward = 0.0\n",
    "\n",
    "    for _ in range(TEST_EPISODE):\n",
    "        reward += Agent.play_episode(test_env)\n",
    "\n",
    "    reward /= TEST_EPISODE\n",
    "    writer.add_scalar(\"reward\", reward , iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(f\"Best reward updated : {best_reward} -> {reward}\")\n",
    "        best_reward = reward\n",
    "\n",
    "    if reward > 0.9:\n",
    "        print(f\"Solved in {iter_no} iterations\")\n",
    "        break \n",
    "writer.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
